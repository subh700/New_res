{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "578bb39b-1c63-455f-bc5a-8d34df28811d",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc127028-217c-443f-aeca-1c2d70986fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Imports and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "import statsmodels.api as sm\n",
    "import os\n",
    "# import os\n",
    "import json\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import keras.backend as K\n",
    "from keras.layers import Layer\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "from colorama import init, Fore, Back, Style\n",
    "from keras.regularizers import l1, l2\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, SGD\n",
    "from tensorflow.keras.layers import ( Dense, Dropout, GRU,Bidirectional, Input)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d1ff166-9bb5-425a-b0be-a68e0d14d99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available foreground colors for logging\n",
    "FORES = [Fore.RED, Fore.GREEN, Fore.YELLOW, Fore.BLUE, Fore.MAGENTA, Fore.CYAN, Fore.WHITE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "551c7893-5ae1-4285-bdb9-918c53d45901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter configurations\n",
    "# model_configs = {'units_1': [128],'units_2': [32],'dropout_rate': [0.1],'learning_rate': [0.001],'regularization': [0.001]}\n",
    "model_configs = {'units_1': [64, 128],'units_2': [32, 64],'dropout_rate': [0.1, 0.2],'learning_rate': [0.001, 0.01],'regularization': [0.001, 0.01]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a0ddb07-7205-49d2-b10f-33a263648d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class MyCustomDataset(Sequence):\n",
    "    def __init__(self, data, labels, batch_size=32, **kwargs):\n",
    "        super().__init__(**kwargs)  # Call the parent constructor with kwargs\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.indexes = np.arange(len(self.data))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Generate one batch of data\n",
    "        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_data = self.data[batch_indexes]\n",
    "        batch_labels = self.labels[batch_indexes]\n",
    "        return batch_data, batch_labels\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Shuffle indexes after each epoch if needed\n",
    "        np.random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33097b7b-b13d-45f8-9bb3-3bb5379dc6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "MODEL_NAME = 'BiGru_Optimized'\n",
    "N_EPOCHS = 300\n",
    "STEPS_PER_EPOCH = 320\n",
    "N_INPUT = 24\n",
    "N_FEATURES = 1\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ffa1cdb-e57b-43cc-a03b-81515bbca2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17 data files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../DataSets\\\\GUJARAT\\\\ANKLESHWAR\\\\H_Ankl_1_2_19-3_12_22-_41_ (copy)_INDEX_Mean.csv',\n",
       " '../DataSets\\\\HARYANA\\\\AMBALA\\\\H_Amba_1_1_19-2_12_22-_29_ (copy)_INDEX_Median.csv',\n",
       " '../DataSets\\\\HARYANA\\\\CHARKHI_DADRI\\\\H_Char_1_3_20-2_12_22-_58_ (copy)_INDEX_Mean.csv',\n",
       " '../DataSets\\\\HARYANA\\\\DHARUHERA\\\\H_Dhar_1_1_19-2_12_22-_43_ (copy)_INDEX_Mean.csv',\n",
       " '../DataSets\\\\HARYANA\\\\FATEHABAD\\\\H_Fate_1_1_19-2_12_22-_32_ (copy)_INDEX_Median.csv',\n",
       " '../DataSets\\\\HARYANA\\\\HISAR\\\\H_Hisa_1_1_19-2_12_22-_10_ (copy)_INDEX_Mean.csv',\n",
       " '../DataSets\\\\HARYANA\\\\JIND\\\\H_Jind_1_1_19-2_12_22-_56_ (copy)_INDEX_Median.csv',\n",
       " '../DataSets\\\\HARYANA\\\\KURUKSHETRA\\\\H_Kuru_1_1_19-2_12_12-_48_ (copy)_INDEX_Mean.csv',\n",
       " '../DataSets\\\\HARYANA\\\\SONIPAT\\\\H_Soni_1_1_19-2_12_22-_21_ (copy)_INDEX_Mean.csv',\n",
       " '../DataSets\\\\HARYANA\\\\YAMUNA_NAGAR\\\\H_Yamu_1_1_19-2_12_22-_30_ (copy)_INDEX_Mean.csv',\n",
       " '../DataSets\\\\MADHYA_PRADESH\\\\SINGRAULI\\\\H_Sing_1_12_17-2_12_22-_48_ (copy)_INDEX_Mean.csv',\n",
       " '../DataSets\\\\PUNJAB\\\\LUDHIANA\\\\H_Ludh_1_5_17-_2_12_22-_07_ (copy)_INDEX_Mean.csv',\n",
       " '../DataSets\\\\RAJASTHAN\\\\BHIWADI\\\\H_Bhiw_1_12_17-2_12_22-_01_ (copy)_INDEX_Mean.csv',\n",
       " '../DataSets\\\\RAJASTHAN\\\\JODHPUR\\\\H_Jodh_1_12_15-2_12_22-_10_ (copy)_INDEX_Mean.csv',\n",
       " '../DataSets\\\\UTTAR_PRADESH\\\\BULANDSHAHR\\\\H_Bula_1_5_18-2_12_22-_38_ (copy)_INDEX_Median.csv',\n",
       " '../DataSets\\\\UTTAR_PRADESH\\\\MUZAFFARNAGAR\\\\H_Muza_1_7_18-2_12_22-_07_ (copy)_INDEX_Mean.csv',\n",
       " '../DataSets\\\\WEST_BENGAL\\\\DURGAPUR\\\\H_Durg_1_12_17-2_12_22-_58_ (copy)_INDEX_Median.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data paths\n",
    "dh = glob.glob('../DataSets/*/*/H*_INDEX_*.csv')\n",
    "print(f\"Found {len(dh)} data files\")\n",
    "\n",
    "# Create initial data list\n",
    "li = []\n",
    "for file_name in dh:\n",
    "    df = pd.read_csv(file_name)\n",
    "    li.append(df)\n",
    "dh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad1a94b-bf7a-4366-9e35-af91c0df3174",
   "metadata": {},
   "source": [
    "# Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "856998bb-070b-4a8f-8bce-d62fc71b709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Data Processing Functions\n",
    "\n",
    "def process_dataset(df, data_path):\n",
    "    \"\"\"Process a single dataset with proper path handling\"\"\"\n",
    "    # Create output directory path\n",
    "    dataso = 'Res_Data/' + data_path.split('/')[2] + '/' + data_path.split('/')[3]\n",
    "    \n",
    "    # Convert date and set index\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    df = df.set_index('DATE')\n",
    "    \n",
    "    # Decompose time series\n",
    "    decomposition = sm.tsa.seasonal_decompose(df['PM2.5'], model='additive')\n",
    "    \n",
    "    # Create DataFrame with components\n",
    "    df1 = pd.DataFrame()\n",
    "    df1['Date'] = df.index\n",
    "    df1['Seasonality'] = decomposition.seasonal.values\n",
    "    df1['Trend'] = decomposition.trend.values\n",
    "    df1['Noise'] = decomposition.resid.values\n",
    "    df1['Original'] = df.iloc[:,-1:].values\n",
    "    \n",
    "    # Process dataframe\n",
    "    df1 = df1.iloc[12:-12,:]\n",
    "    df1 = df1.reset_index()\n",
    "    df1 = df1.iloc[:,1:]\n",
    "    df1 = df1.set_index('Date')\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(dataso, exist_ok=True)\n",
    "    \n",
    "    # Save decomposed data\n",
    "    df1.to_csv(dataso + '/' + data_path.split('/')[4])\n",
    "    \n",
    "    return df1, dataso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "488d60c2-6311-4b27-972d-9287271dd6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_component_data(df, component_name, dataso):\n",
    "    \"\"\"Prepare data for a specific component, checking for existing CSV files.\"\"\"\n",
    "    # Define the path for the component CSV file\n",
    "    component_file_path = os.path.join(dataso, f'{component_name}.csv')\n",
    "    \n",
    "    # Check if the component CSV file already exists\n",
    "    if os.path.exists(component_file_path):\n",
    "        print(f\"Loading existing data for {component_name} from {component_file_path}\")\n",
    "        component_df = pd.read_csv(component_file_path, index_col=0)\n",
    "    else:\n",
    "        print(f\"Creating new data for {component_name}...\")\n",
    "        # Select appropriate component data\n",
    "        if component_name == 'Seasonality':\n",
    "            component_df = df.iloc[:, :1]\n",
    "        elif component_name == 'Trend':\n",
    "            component_df = df.iloc[:, 1:2]\n",
    "        elif component_name == 'Noise':\n",
    "            component_df = df.iloc[:, 2:3]\n",
    "        else:  # Original\n",
    "            component_df = df.iloc[:, 3:4]\n",
    "        \n",
    "        # Save component data\n",
    "        component_df.to_csv(component_file_path)\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled = scaler.fit_transform(component_df.values)\n",
    "    \n",
    "    # Split data\n",
    "    train, test = train_test_split(scaled, test_size=0.30, shuffle=False)\n",
    "    train_target = train[:]\n",
    "    test_target = test[:]\n",
    "    \n",
    "    # Create generators\n",
    "    train_generator = TimeseriesGenerator(\n",
    "        train, train_target, \n",
    "        length=N_INPUT, \n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "    test_generator = TimeseriesGenerator(\n",
    "        test, test_target, \n",
    "        length=N_INPUT, \n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    return train_generator, test_generator, scaler, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bc0e1e6-5a49-4da1-bb9b-7bcabba240fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Attention Layer\n",
    "class attention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(attention,self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self,input_shape):\n",
    "        self.W = self.add_weight(name='attention_weight', shape=(input_shape[-1],1), initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(name='attention_bias', shape=(input_shape[1],1), initializer='zeros', trainable=True)        \n",
    "        super(attention, self).build(input_shape)\n",
    "    \n",
    "    def call(self,x):\n",
    "        # Alignment scores. Pass them through tanh function\n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "        # Remove dimension of size 1\n",
    "        e = K.squeeze(e, axis=-1)   \n",
    "        # Compute the weights\n",
    "        alpha = K.softmax(e)\n",
    "        # Reshape to tensorFlow format\n",
    "        alpha = K.expand_dims(alpha, axis=-1)\n",
    "        # Compute the context vector\n",
    "        context = x * alpha\n",
    "        context = K.sum(context, axis=1)\n",
    "        return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b22e1021-1524-4a17-8133-f1bf98ae51cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def simple_grid_search(train_generator, test_generator, configs):\n",
    "#     \"\"\"Perform manual grid search for hyperparameters\"\"\"\n",
    "#     best_val_loss = float('inf')\n",
    "#     best_params = None\n",
    "#     best_model = None\n",
    "    \n",
    "#     # Try different combinations\n",
    "#     for units_1 in configs['units_1']:\n",
    "#         for units_2 in configs['units_2']:\n",
    "#             for dropout in configs['dropout_rate']:\n",
    "#                 for lr in configs['learning_rate']:\n",
    "#                     for reg in configs['regularization']:\n",
    "#                         print(f\"\\nTrying parameters: units1={units_1}, units2={units_2}, \" f\"dropout={dropout}, lr={lr}, reg={reg}\")\n",
    "                        \n",
    "#                         # Create and train model\n",
    "#                         model = create_optimized_model(units_1=units_1,units_2=units_2,dropout_rate=dropout,learning_rate=lr,regularization=reg)\n",
    "                        \n",
    "#                         # Early stopping\n",
    "#                         es = EarlyStopping(monitor='val_loss', mode='min', patience=5, verbose=0)\n",
    "                        \n",
    "#                         # Train for fewer epochs during search\n",
    "#                         history = model.fit(train_generator,validation_data=test_generator,epochs=50, steps_per_epoch=STEPS_PER_EPOCH,shuffle=False,callbacks=[es],verbose=0)\n",
    "                        \n",
    "#                         val_loss = min(history.history['val_loss'])\n",
    "                        \n",
    "#                         if val_loss < best_val_loss:\n",
    "#                             best_val_loss = val_loss\n",
    "#                             best_params = {'units_1': units_1,'units_2': units_2,'dropout_rate': dropout,'learning_rate': lr,'regularization': reg}\n",
    "#                             best_model = model\n",
    "                            \n",
    "#                         print(f\"Val Loss: {val_loss:.4f}\")\n",
    "                        \n",
    "#     return best_params, best_model, best_val_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def simple_grid_search(train_generator, test_generator, configs, model_dir, component):\n",
    "#     \"\"\"Perform manual grid search for hyperparameters\"\"\"\n",
    "#     best_val_loss = float('inf')\n",
    "#     best_params = None\n",
    "#     best_model = None\n",
    "    \n",
    "#     # Check if a model already exists\n",
    "#     model_path = f'{model_dir}/best_model_{component}.keras'\n",
    "#     if os.path.exists(model_path):\n",
    "#         print(f\"Loading existing model for {component} from {model_path}\")\n",
    "#         best_model = tf.keras.models.load_model(model_path)\n",
    "#         best_val_loss = float('inf')  # Set to inf to ensure it trains\n",
    "#         return best_params, best_model, best_val_loss\n",
    "    \n",
    "#     # Try different combinations\n",
    "#     for units_1 in configs['units_1']:\n",
    "#         for units_2 in configs['units_2']:\n",
    "#             for dropout in configs['dropout_rate']:\n",
    "#                 for lr in configs['learning_rate']:\n",
    "#                     for reg in configs['regularization']:\n",
    "#                         print(f\"\\nTrying parameters: units1={units_1}, units2={units_2}, \" f\"dropout={dropout}, lr={lr}, reg={reg}\")\n",
    "                        \n",
    "#                         # Create and train model\n",
    "#                         model = create_optimized_model(units_1=units_1,units_2=units_2,dropout_rate=dropout,learning_rate=lr,regularization=reg)\n",
    "                        \n",
    "#                         # Early stopping\n",
    "#                         es = EarlyStopping(monitor='val_loss', mode='min', patience=5, verbose=0)\n",
    "                        \n",
    "#                         # Train for fewer epochs during search\n",
    "#                         history = model.fit(train_generator,validation_data=test_generator,epochs=50, steps_per_epoch=STEPS_PER_EPOCH,shuffle=False,callbacks=[es],verbose=0)\n",
    "                        \n",
    "#                         val_loss = min(history.history['val_loss'])\n",
    "                        \n",
    "#                         if val_loss < best_val_loss:\n",
    "#                             best_val_loss = val_loss\n",
    "#                             best_params = {'units_1': units_1,'units_2': units_2,'dropout_rate': dropout,'learning_rate': lr,'regularization': reg}\n",
    "#                             best_model = model\n",
    "                            \n",
    "#                         print(f\"Val Loss: {val_loss:.4f}\")\n",
    "                        \n",
    "#     return best_params, best_model, best_val_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def simple_grid_search(train_generator, test_generator, configs, model_dir, component):\n",
    "    \"\"\"Perform manual grid search for hyperparameters\"\"\"\n",
    "    best_val_loss = float('inf')\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    # Check if a model already exists\n",
    "    model_path = f'{model_dir}/best_model_{component}.keras'\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Loading existing model for {component} from {model_path}\")\n",
    "        best_model = tf.keras.models.load_model(model_path)\n",
    "        # Return None for best_params since we are not performing a search\n",
    "        return None, best_model, best_val_loss\n",
    "    \n",
    "    # Try different combinations\n",
    "    for units_1 in configs['units_1']:\n",
    "        for units_2 in configs['units_2']:\n",
    "            for dropout in configs['dropout_rate']:\n",
    "                for lr in configs['learning_rate']:\n",
    "                    for reg in configs['regularization']:\n",
    "                        print(f\"\\nTrying parameters: units1={units_1}, units2={units_2}, \" f\"dropout={dropout}, lr={lr}, reg={reg}\")\n",
    "                        \n",
    "                        # Create and train model\n",
    "                        model = create_optimized_model(units_1=units_1,units_2=units_2,dropout_rate=dropout,learning_rate=lr,regularization=reg)\n",
    "                        \n",
    "                        # Early stopping\n",
    "                        es = EarlyStopping(monitor='val_loss', mode='min', patience=5, verbose=0)\n",
    "                        \n",
    "                        # Train for fewer epochs during search\n",
    "                        history = model.fit(train_generator, validation_data=test_generator, epochs=50, steps_per_epoch=STEPS_PER_EPOCH, shuffle=False, callbacks=[es], verbose=0)\n",
    "                        \n",
    "                        val_loss = min(history.history['val_loss'])\n",
    "                        \n",
    "                        if val_loss < best_val_loss:\n",
    "                            best_val_loss = val_loss\n",
    "                            best_params = {'units_1': units_1, 'units_2': units_2, 'dropout_rate': dropout, 'learning_rate': lr, 'regularization': reg}\n",
    "                            best_model = model\n",
    "                            \n",
    "                        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # print(f\"Best parameters: {best_params}, Best validation loss: {best_val_loss:.4f}\")  # Debugging line\n",
    "    print(f\"Best parameters: {best_params}, Best validation loss: {best_val_loss:.4f}\")  # Debugging line\n",
    "    return best_params, best_model, best_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9b7def-bb24-4425-9178-4be12829eaf0",
   "metadata": {},
   "source": [
    "# Model Definition and Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5cf0bef-7ed2-4fc3-ba3f-868ea5a11b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Model Definition and Architecture\n",
    "\n",
    "def create_optimized_model(units_1=128, units_2=64, dropout_rate=0.2, \n",
    "                         learning_rate=0.001, regularization=0.001):\n",
    "    \"\"\"Create the optimized BiGRU model with specified hyperparameters\"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(Input(shape=(N_INPUT, N_FEATURES)))  # Define input shape here\n",
    "\n",
    "    # First Bidirectional GRU Layer\n",
    "    model.add(Bidirectional(GRU(units=units_1, return_sequences=True, \n",
    "                                 kernel_regularizer=l1(regularization))))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Second Bidirectional GRU Layer\n",
    "    model.add(Bidirectional(GRU(units=units_2, return_sequences=False, \n",
    "                                 kernel_regularizer=l1(regularization))))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Dense layers for final prediction\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Compile model\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f59e3ec8-dc82-45fb-a251-dc834b325480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_callbacks(model_dir, component):\n",
    "    \"\"\"Create callbacks for model training\"\"\"\n",
    "    callbacks = [\n",
    "        # Early Stopping\n",
    "        EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=12),\n",
    "        \n",
    "        # Model Checkpoint\n",
    "        ModelCheckpoint(filepath=f'{model_dir}/best_model_{component}.keras',monitor='val_loss',mode='min',save_best_only=True,verbose=1),\n",
    "        \n",
    "        # Reduce Learning Rate\n",
    "        ReduceLROnPlateau(monitor='val_loss',factor=0.2,patience=5,min_lr=0.0001,verbose=1)\n",
    "    ]\n",
    "    \n",
    "    return callbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "941b5ac6-877b-4a42-8f86-4b6daf4b0354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_predictions(prediction, pre_train, test, train, n_input, scaler):\n",
    "    \"\"\"Process model predictions\"\"\"\n",
    "    # Process test predictions\n",
    "    test1 = test[n_input:,:]\n",
    "    prediction1 = np.concatenate((prediction, test1), axis=1) if prediction.size > 0 and test1.size > 0 else np.array([])\n",
    "    rescaled_prediction = scaler.inverse_transform(prediction1) if prediction1.size > 0 else np.array([])\n",
    "    rescaled_prediction_test = rescaled_prediction[:,0] if rescaled_prediction.size > 0 else np.array([])\n",
    "    \n",
    "    # Process train predictions\n",
    "    train1 = train[n_input:,:]\n",
    "    pre_train1 = np.concatenate((pre_train, train1), axis=1) if pre_train.size > 0 and train1.size > 0 else np.array([])\n",
    "    rescaled_prediction_trai = scaler.inverse_transform(pre_train1) if pre_train1.size > 0 else np.array([])\n",
    "    rescaled_prediction_train = rescaled_prediction_trai[:,0] if rescaled_prediction_trai.size > 0 else np.array([])\n",
    "    \n",
    "    return rescaled_prediction_test, rescaled_prediction_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "310d453e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_predictions(rescaled_prediction_test, rescaled_prediction_train, \n",
    "#                     component, model_dir):\n",
    "#     \"\"\"Save model predictions\"\"\"\n",
    "#     # Create DataFrame for real and predicted values\n",
    "#     if component == 'Original':\n",
    "#         test_pr = pd.DataFrame(rescaled_prediction_test, columns=['Predicted']) if rescaled_prediction_test.size > 0 else pd.DataFrame()\n",
    "#         train_pr = pd.DataFrame(rescaled_prediction_train, columns=['Predicted']) if rescaled_prediction_train.size > 0 else pd.DataFrame()\n",
    "#         # Add real data column\n",
    "#         test_pr['Real'] = test[:, 0]  # Assuming the first column is the real data\n",
    "#         train_pr['Real'] = train[:, 0]  # Assuming the first column is the real data\n",
    "#         test_pr['Date'] = pd.date_range(start='2021-01-01', periods=len(test_pr))  # Replace with actual date range\n",
    "#         train_pr['Date'] = pd.date_range(start='2021-01-01', periods=len(train_pr))  # Replace with actual date range\n",
    "#     else:\n",
    "#         # Load existing predictions if available\n",
    "#         test_pr = pd.read_csv(f'{model_dir}/test_pre.csv', index_col=0) \\\n",
    "#                  if os.path.exists(f'{model_dir}/test_pre.csv') \\\n",
    "#                  else pd.DataFrame()\n",
    "#         train_pr = pd.read_csv(f'{model_dir}/train_pre.csv', index_col=0) \\\n",
    "#                   if os.path.exists(f'{model_dir}/train_pre.csv') \\\n",
    "#                   else pd.DataFrame()\n",
    "        \n",
    "#         # Add new predictions if they are not empty\n",
    "#         if rescaled_prediction_test.size > 0:\n",
    "#             test_pr[f'{component[0]}_Predicted'] = rescaled_prediction_test\n",
    "#         if rescaled_prediction_train.size > 0:\n",
    "#             train_pr[f'{component[0]}_Predicted'] = rescaled_prediction_train\n",
    "        \n",
    "#         # Add real data column\n",
    "#         test_pr['Real'] = test[:, 0]  # Assuming the first column is the real data\n",
    "#         train_pr['Real'] = train[:, 0]  # Assuming the first column is the real data\n",
    "#         test_pr['Date'] = pd.date_range(start='2021-01-01', periods=len(test_pr))  # Replace with actual date range\n",
    "#         train_pr['Date'] = pd.date_range(start='2021-01-01', periods=len(train_pr))  # Replace with actual date range\n",
    "    \n",
    "#     # Save to files\n",
    "#     test_pr.to_csv(f'{model_dir}/{component}_test_predictions.csv', index=False)\n",
    "#     train_pr.to_csv(f'{model_dir}/{component}_train_predictions.csv', index=False)\n",
    "    \n",
    "#     return test_pr, train_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45c48b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_predictions(rescaled_prediction_test, rescaled_prediction_train, \n",
    "#                     component, model_dir, real_test, real_train):\n",
    "#     \"\"\"Save model predictions\"\"\"\n",
    "#     # Create DataFrame for real and predicted values\n",
    "#     test_pr = pd.DataFrame({\n",
    "#         'Date': pd.date_range(start='2021-01-01', periods=len(rescaled_prediction_test)),  # Replace with actual date range\n",
    "#         'Real': real_test[:, 0] if real_test.size > 0 else np.array([]),  # Assuming the first column is the real data\n",
    "#         'Predicted': rescaled_prediction_test.flatten() if rescaled_prediction_test.size > 0 else np.array([])  # Flatten in case of 2D array\n",
    "#     }) if rescaled_prediction_test.size > 0 else pd.DataFrame()\n",
    "\n",
    "#     train_pr = pd.DataFrame({\n",
    "#         'Date': pd.date_range(start='2021-01-01', periods=len(rescaled_prediction_train)),  # Replace with actual date range\n",
    "#         'Real': real_train[:, 0] if real_train.size > 0 else np.array([]),  # Assuming the first column is the real data\n",
    "#         'Predicted': rescaled_prediction_train.flatten() if rescaled_prediction_train.size > 0 else np.array([])  # Flatten in case of 2D array\n",
    "#     }) if rescaled_prediction_train.size > 0 else pd.DataFrame()\n",
    "\n",
    "#     # Save to files\n",
    "#     test_pr.to_csv(f'{model_dir}/{component}_test_predictions.csv', index=False)\n",
    "#     train_pr.to_csv(f'{model_dir}/{component}_train_predictions.csv', index=False)\n",
    "    \n",
    "#     return test_pr, train_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73106319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(rescaled_prediction_test, rescaled_prediction_train, \n",
    "                     component, model_dir):\n",
    "    \"\"\"Save model predictions\"\"\"\n",
    "    # Create DataFrames for predictions\n",
    "    if component == 'Original':\n",
    "        test_pr = pd.DataFrame(rescaled_prediction_test, columns=['Original_Predicted']) if rescaled_prediction_test.size > 0 else pd.DataFrame()\n",
    "        train_pr = pd.DataFrame(rescaled_prediction_train, columns=['Original_Predicted']) if rescaled_prediction_train.size > 0 else pd.DataFrame()\n",
    "    else:\n",
    "        # Load existing predictions if available\n",
    "        test_pr = pd.read_csv(f'{model_dir}/test_pre.csv', index_col=0) \\\n",
    "                 if os.path.exists(f'{model_dir}/test_pre.csv') \\\n",
    "                 else pd.DataFrame()\n",
    "        train_pr = pd.read_csv(f'{model_dir}/train_pre.csv', index_col=0) \\\n",
    "                  if os.path.exists(f'{model_dir}/train_pre.csv') \\\n",
    "                  else pd.DataFrame()\n",
    "        \n",
    "        # Add new predictions if they are not empty\n",
    "        if rescaled_prediction_test.size > 0:\n",
    "            test_pr[f'{component}_Predicted'] = rescaled_prediction_test\n",
    "        if rescaled_prediction_train.size > 0:\n",
    "            train_pr[f'{component}_Predicted'] = rescaled_prediction_train\n",
    "    \n",
    "    # Save to files with component-specific names\n",
    "    test_pr.to_csv(f'{model_dir}/{component}_test_predictions.csv', index=False)\n",
    "    train_pr.to_csv(f'{model_dir}/{component}_train_predictions.csv', index=False)\n",
    "    \n",
    "    return test_pr, train_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "440c8bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_predictions(rescaled_prediction_test, rescaled_prediction_train, \n",
    "#                     component, model_dir):\n",
    "#     \"\"\"Save model predictions\"\"\"\n",
    "#     if component == 'Original':\n",
    "#         test_pr = pd.DataFrame(rescaled_prediction_test, columns=['o_prid']) if rescaled_prediction_test.size > 0 else pd.DataFrame()\n",
    "#         train_pr = pd.DataFrame(rescaled_prediction_train, columns=['o_prid']) if rescaled_prediction_train.size > 0 else pd.DataFrame()\n",
    "#     else:\n",
    "#         # Load existing predictions if available\n",
    "#         test_pr = pd.read_csv(f'{model_dir}/test_pre.csv', index_col=0) \\\n",
    "#                  if os.path.exists(f'{model_dir}/test_pre.csv') \\\n",
    "#                  else pd.DataFrame()\n",
    "#         train_pr = pd.read_csv(f'{model_dir}/train_pre.csv', index_col=0) \\\n",
    "#                   if os.path.exists(f'{model_dir}/train_pre.csv') \\\n",
    "#                   else pd.DataFrame()\n",
    "        \n",
    "#         # Add new predictions if they are not empty\n",
    "#         if rescaled_prediction_test.size > 0:\n",
    "#             test_pr[f'{component[0]}_prid'] = rescaled_prediction_test\n",
    "#         if rescaled_prediction_train.size > 0:\n",
    "#             train_pr[f'{component[0]}_prid'] = rescaled_prediction_train\n",
    "#     # ... existing code ...\n",
    "#     # Save to files\n",
    "#     test_pr.to_csv(f'{model_dir}/{component}_test_predictions.csv', index=False)  # Updated to save with component name\n",
    "#     train_pr.to_csv(f'{model_dir}/{component}_train_predictions.csv', index=False)  # Updated to save with component name\n",
    "#     return test_pr, train_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "620d4ea2-831b-4741-ae46-7668d20bf497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_predictions(rescaled_prediction_test, rescaled_prediction_train, \n",
    "#                     component, model_dir):\n",
    "#     \"\"\"Save model predictions\"\"\"\n",
    "#     if component == 'Original':\n",
    "#         test_pr = pd.DataFrame(rescaled_prediction_test, columns=['o_prid']) if rescaled_prediction_test.size > 0 else pd.DataFrame()\n",
    "#         train_pr = pd.DataFrame(rescaled_prediction_train, columns=['o_prid']) if rescaled_prediction_train.size > 0 else pd.DataFrame()\n",
    "#     else:\n",
    "#         # Load existing predictions if available\n",
    "#         test_pr = pd.read_csv(f'{model_dir}/test_pre.csv', index_col=0) \\\n",
    "#                  if os.path.exists(f'{model_dir}/test_pre.csv') \\\n",
    "#                  else pd.DataFrame()\n",
    "#         train_pr = pd.read_csv(f'{model_dir}/train_pre.csv', index_col=0) \\\n",
    "#                   if os.path.exists(f'{model_dir}/train_pre.csv') \\\n",
    "#                   else pd.DataFrame()\n",
    "        \n",
    "#         # Add new predictions if they are not empty\n",
    "#         if rescaled_prediction_test.size > 0:\n",
    "#             test_pr[f'{component[0]}_prid'] = rescaled_prediction_test\n",
    "#         if rescaled_prediction_train.size > 0:\n",
    "#             train_pr[f'{component[0]}_prid'] = rescaled_prediction_train\n",
    "    \n",
    "#     # Save to files\n",
    "#     test_pr.to_csv(f'{model_dir}/test_pre.csv')\n",
    "#     train_pr.to_csv(f'{model_dir}/train_pre.csv')\n",
    "    \n",
    "#     return test_pr, train_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b12973a2-2ba9-4beb-a1f6-fec77207365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_history(history, model_dir, component):\n",
    "    \"\"\"Save model training history\"\"\"\n",
    "    hist_df = pd.DataFrame(history.history)\n",
    "    hist_df.to_csv(f'{model_dir}/{component}_history.csv')\n",
    "    \n",
    "    # Create and save training plots\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{component} Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot MAE\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='Training MAE')\n",
    "    plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "    plt.title(f'{component} Model MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{model_dir}/{component}_training_history.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebe56f2-95b5-4b80-8ddb-1f89bb9edae4",
   "metadata": {},
   "source": [
    "# Training and Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6996b4b8-2ac8-4d70-9dd0-24edee2fb4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset 1/17\n",
      "\u001b[31mDATASET----------------../DataSets\\GUJARAT\\ANKLESHWAR\\H_Ankl_1_2_19-3_12_22-_41_ (copy)_INDEX_Mean.csv\n",
      "\n",
      "Processing Seasonality component...\n",
      "Loading existing data for Seasonality from Res_Data\\GUJARAT\\ANKLESHWAR\\Seasonality.csv\n",
      "Data shapes - Train: (23457, 1), Test: (10054, 1)\n",
      "Performing hyperparameter search...\n",
      "\n",
      "Trying parameters: units1=128, units2=32, dropout=0.1, lr=0.001, reg=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vp123\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.0125\n",
      "Best parameters: {'units_1': 128, 'units_2': 32, 'dropout_rate': 0.1, 'learning_rate': 0.001, 'regularization': 0.001}, Best validation loss: 0.0125\n",
      "\n",
      "Best parameters found:\n",
      "units_1: 128\n",
      "units_2: 32\n",
      "dropout_rate: 0.1\n",
      "learning_rate: 0.001\n",
      "regularization: 0.001\n",
      "Best validation loss: 0.0125\n",
      "\n",
      "Training final model with best parameters...\n",
      "\u001b[1m319/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.5403 - mae: 0.1670\n",
      "Epoch 1: val_loss improved from inf to 0.07557, saving model to Res_Data\\GUJARAT\\ANKLESHWAR/BiGru_Opt/best_model_Seasonality.keras\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.07557, saving model to Res_Data\\GUJARAT\\ANKLESHWAR/BiGru_Opt/best_model_Seasonality.keras\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 91ms/step - loss: 1.5351 - mae: 0.1667 - val_loss: 0.0756 - val_mae: 0.0484 - learning_rate: 0.0010\n",
      "Best hyperparameters saved to Res_Data\\GUJARAT\\ANKLESHWAR/BiGru_Opt/best_hyperparameters_Seasonality.txt\n",
      "Generating predictions...\n",
      "\u001b[1m10030/10030\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step\n",
      "\u001b[1m23433/23433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 3ms/step\n",
      "Shapes - Test Predictions: (10030,), Train Predictions: (23433,)\n",
      "Completed Seasonality component\n",
      "\n",
      "Processing Trend component...\n",
      "Loading existing data for Trend from Res_Data\\GUJARAT\\ANKLESHWAR\\Trend.csv\n",
      "Data shapes - Train: (23457, 1), Test: (10054, 1)\n",
      "Performing hyperparameter search...\n",
      "\n",
      "Trying parameters: units1=128, units2=32, dropout=0.1, lr=0.001, reg=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vp123\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.0263\n",
      "Best parameters: {'units_1': 128, 'units_2': 32, 'dropout_rate': 0.1, 'learning_rate': 0.001, 'regularization': 0.001}, Best validation loss: 0.0263\n",
      "\n",
      "Best parameters found:\n",
      "units_1: 128\n",
      "units_2: 32\n",
      "dropout_rate: 0.1\n",
      "learning_rate: 0.001\n",
      "regularization: 0.001\n",
      "Best validation loss: 0.0263\n",
      "\n",
      "Training final model with best parameters...\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.2984 - mae: 0.0567\n",
      "Epoch 1: val_loss improved from inf to 0.03154, saving model to Res_Data\\GUJARAT\\ANKLESHWAR/BiGru_Opt/best_model_Trend.keras\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.03154, saving model to Res_Data\\GUJARAT\\ANKLESHWAR/BiGru_Opt/best_model_Trend.keras\n",
      "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 93ms/step - loss: 1.2959 - mae: 0.0567 - val_loss: 0.0315 - val_mae: 0.1184 - learning_rate: 0.0010\n",
      "Best hyperparameters saved to Res_Data\\GUJARAT\\ANKLESHWAR/BiGru_Opt/best_hyperparameters_Trend.txt\n",
      "Generating predictions...\n",
      "\u001b[1m10030/10030\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step\n",
      "\u001b[1m23433/23433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 3ms/step\n",
      "Shapes - Test Predictions: (10030,), Train Predictions: (23433,)\n",
      "Completed Trend component\n",
      "\n",
      "Processing Noise component...\n",
      "Loading existing data for Noise from Res_Data\\GUJARAT\\ANKLESHWAR\\Noise.csv\n",
      "Data shapes - Train: (23457, 1), Test: (10054, 1)\n",
      "Performing hyperparameter search...\n",
      "\n",
      "Trying parameters: units1=128, units2=32, dropout=0.1, lr=0.001, reg=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vp123\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 160\u001b[0m\n\u001b[0;32m    157\u001b[0m             best_params[param] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(value) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m value \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mint\u001b[39m(value)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;66;03m# Perform hyperparameter search if no file exists\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m     best_params, best_model, best_val_loss \u001b[38;5;241m=\u001b[39m \u001b[43msimple_grid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_configs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomponent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# Check if best_params is None before proceeding\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m best_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[10], line 115\u001b[0m, in \u001b[0;36msimple_grid_search\u001b[1;34m(train_generator, test_generator, configs, model_dir, component)\u001b[0m\n\u001b[0;32m    112\u001b[0m es \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Train for fewer epochs during search\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSTEPS_PER_EPOCH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mes\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_loss \u001b[38;5;241m<\u001b[39m best_val_loss:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:392\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_epoch_iterator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_epoch_iterator \u001b[38;5;241m=\u001b[39m TFEpochIterator(\n\u001b[0;32m    383\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m    384\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    391\u001b[0m     )\n\u001b[1;32m--> 392\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    402\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    404\u001b[0m }\n\u001b[0;32m    405\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:481\u001b[0m, in \u001b[0;36mTensorFlowTrainer.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    480\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m--> 481\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    482\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_end(step, logs)\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_evaluating:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:219\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopt_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution), iterator\n\u001b[0;32m    223\u001b[0m     ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\data\\ops\\optional_ops.py:186\u001b[0m, in \u001b[0;36m_OptionalImpl.get_value\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptionalGetValue\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    184\u001b[0m                     [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor]) \u001b[38;5;28;01mas\u001b[39;00m scope:\n\u001b[0;32m    185\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 186\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mgen_optional_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptional_get_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variant_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscope\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstructure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_flat_tensor_types\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_element_spec\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstructure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_flat_tensor_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_element_spec\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m   \u001b[38;5;66;03m# NOTE: We do not colocate the deserialization of composite tensors\u001b[39;00m\n\u001b[0;32m    193\u001b[0m   \u001b[38;5;66;03m# because not all ops are guaranteed to have non-GPU kernels.\u001b[39;00m\n\u001b[0;32m    194\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m structure\u001b[38;5;241m.\u001b[39mfrom_tensor_list(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec, result)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\ops\\gen_optional_ops.py:95\u001b[0m, in \u001b[0;36moptional_get_value\u001b[1;34m(optional, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m     94\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 95\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOptionalGetValue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_types\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_shapes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m     99\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Section 4: Training and Evaluation Loop\n",
    "\n",
    "# Define components to process\n",
    "components = ['Seasonality', 'Trend', 'Noise', 'Original']\n",
    "\n",
    "# Main training loop\n",
    "for ds in range(len(dh)):\n",
    "    print(f\"\\nProcessing dataset {ds+1}/{len(dh)}\")\n",
    "    print(FORES[ds % len(FORES)] + f'DATASET----------------{dh[ds]}')\n",
    "    \n",
    "    try:\n",
    "        # Process dataset\n",
    "        df1, dataso = process_dataset(li[ds], dh[ds])\n",
    "        \n",
    "        # Create model directory\n",
    "        model_dir = dataso + '/' + MODEL_NAME\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "        # Store results for this dataset\n",
    "        dataset_results = {\n",
    "            'dataset': dh[ds],\n",
    "            'components': {}\n",
    "        }\n",
    "        \n",
    "        # Process each component\n",
    "        for component in components:\n",
    "            print(f\"\\nProcessing {component} component...\")\n",
    "            \n",
    "            try:\n",
    "                # Prepare data\n",
    "                train_generator, test_generator, scaler, train, test = prepare_component_data(df1, component, dataso)\n",
    "                \n",
    "                print(f\"Data shapes - Train: {train.shape}, Test: {test.shape}\")\n",
    "                \n",
    "                # Perform grid search\n",
    "                print(\"Performing hyperparameter search...\")\n",
    "                # # best_params, best_model, best_val_loss = simple_grid_search(train_generator, test_generator, model_configs)\n",
    "                # best_params, best_model, best_val_loss = simple_grid_search(train_generator, test_generator, model_configs, model_dir, component)\n",
    "                \n",
    "                # # Check if best_params is None before proceeding\n",
    "                # if best_params is not None:\n",
    "                #     print(\"\\nBest parameters found:\")\n",
    "                #     for param, value in best_params.items():\n",
    "                #         print(f\"{param}: {value}\")\n",
    "                #     print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "                # else:\n",
    "                #     print(f\"No valid parameters found for {component}. Skipping training for this component.\")\n",
    "                #     continue  # Skip to the next component\n",
    "\n",
    "\n",
    "                # # print(\"\\nBest parameters found:\")\n",
    "                # # for param, value in best_params.items():\n",
    "                # #     print(f\"{param}: {value}\")\n",
    "                # # print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "                \n",
    "                # # Create callbacks\n",
    "                # callbacks = create_callbacks(model_dir, component)\n",
    "                \n",
    "                # # Train final model with best parameters\n",
    "                # print(\"\\nTraining final model with best parameters...\")\n",
    "                # final_model = create_optimized_model(**best_params)\n",
    "                \n",
    "                # # Train model\n",
    "                # history = final_model.fit(train_generator,validation_data=test_generator,epochs=N_EPOCHS,steps_per_epoch=STEPS_PER_EPOCH,shuffle=False,callbacks=callbacks,verbose=1)\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "                # Inside the training loop, after calling simple_grid_search\n",
    "                # best_params, best_model, best_val_loss = simple_grid_search(train_generator, test_generator, model_configs, model_dir, component)\n",
    "\n",
    "                # # Check if best_params is None before proceeding\n",
    "                # if best_params is not None:\n",
    "                #     print(\"\\nBest parameters found:\")\n",
    "                #     for param, value in best_params.items():\n",
    "                #         print(f\"{param}: {value}\")\n",
    "                #     print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "                # else:\n",
    "                #     # Set default parameters if no valid parameters were found\n",
    "                #     best_params = {'units_1': 64, 'units_2': 32, 'dropout_rate': 0.1, 'learning_rate': 0.001, 'regularization': 0.001}\n",
    "                #     print(\"No valid parameters found for Seasonality. Using default parameters.\")\n",
    "                    \n",
    "                # # Create callbacks\n",
    "                # callbacks = create_callbacks(model_dir, component)\n",
    "\n",
    "                # # Train final model with best parameters\n",
    "                # print(\"\\nTraining final model with best parameters...\")\n",
    "                # final_model = create_optimized_model(**best_params)\n",
    "\n",
    "                # # Train model\n",
    "                # history = final_model.fit(train_generator, validation_data=test_generator, epochs=N_EPOCHS, steps_per_epoch=STEPS_PER_EPOCH, shuffle=False, callbacks=callbacks, verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # # Inside the training loop, after calling simple_grid_search\n",
    "                # best_params, best_model, best_val_loss = simple_grid_search(train_generator, test_generator, model_configs, model_dir, component)\n",
    "\n",
    "                # # Check if best_params is None before proceeding\n",
    "                # if best_params is not None:\n",
    "                #     print(\"\\nBest parameters found:\")\n",
    "                #     for param, value in best_params.items():\n",
    "                #         print(f\"{param}: {value}\")\n",
    "                #     print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "                # else:\n",
    "                #     # Set internal default parameters if no valid parameters were found\n",
    "                #     internal_params = {\n",
    "                #         'units_1': 64,\n",
    "                #         'units_2': 32,\n",
    "                #         'dropout_rate': 0.1,\n",
    "                #         'learning_rate': 0.001,\n",
    "                #         'regularization': 0.001\n",
    "                #     }\n",
    "                #     print(\"No valid parameters found for Seasonality. Using internal default parameters.\")\n",
    "                    \n",
    "                #     # Use internal parameters for model creation\n",
    "                #     best_params = internal_params\n",
    "\n",
    "                # # Create callbacks\n",
    "                # callbacks = create_callbacks(model_dir, component)\n",
    "\n",
    "                # # Add ModelCheckpoint to save the best model\n",
    "                # checkpoint_path = f\"{model_dir}/best_model_{component}.keras\"\n",
    "                # model_checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "                # callbacks.append(model_checkpoint)\n",
    "\n",
    "                # # Train final model with best parameters\n",
    "                # print(\"\\nTraining final model with best parameters...\")\n",
    "                # final_model = create_optimized_model(**best_params)\n",
    "\n",
    "                # # Train model\n",
    "                # history = final_model.fit(train_generator, validation_data=test_generator, epochs=N_EPOCHS, steps_per_epoch=STEPS_PER_EPOCH, shuffle=False, callbacks=callbacks, verbose=1)\n",
    "\n",
    "                # # Save the best hyperparameters to a file\n",
    "                # with open(f\"{model_dir}/best_hyperparameters_{component}.txt\", \"w\") as f:\n",
    "                #     f.write(\"Best Hyperparameters:\\n\")\n",
    "                #     for param, value in best_params.items():\n",
    "                #         f.write(f\"{param}: {value}\\n\")\n",
    "\n",
    "                # print(f\"Best hyperparameters saved to {model_dir}/best_hyperparameters_{component}.txt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # Define the path for the best hyperparameters file\n",
    "                hyperparams_file = f\"{model_dir}/best_hyperparameters_{component}.txt\"\n",
    "\n",
    "                # Check if the best hyperparameters file exists\n",
    "                if os.path.exists(hyperparams_file):\n",
    "                    print(f\"Loading existing best hyperparameters for {component} from {hyperparams_file}\")\n",
    "                    with open(hyperparams_file, \"r\") as f:\n",
    "                        # Read the hyperparameters from the file\n",
    "                        best_params = {}\n",
    "                        for line in f.readlines()[1:]:  # Skip the first line\n",
    "                            param, value = line.strip().split(\": \")\n",
    "                            best_params[param] = float(value) if '.' in value else int(value)\n",
    "                else:\n",
    "                    # Perform hyperparameter search if no file exists\n",
    "                    best_params, best_model, best_val_loss = simple_grid_search(train_generator, test_generator, model_configs, model_dir, component)\n",
    "\n",
    "                    # Check if best_params is None before proceeding\n",
    "                    if best_params is not None:\n",
    "                        print(\"\\nBest parameters found:\")\n",
    "                        for param, value in best_params.items():\n",
    "                            print(f\"{param}: {value}\")\n",
    "                        print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "                    else:\n",
    "                        # Set internal default parameters if no valid parameters were found\n",
    "                        internal_params = {\n",
    "                            'units_1': 64,\n",
    "                            'units_2': 32,\n",
    "                            'dropout_rate': 0.1,\n",
    "                            'learning_rate': 0.001,\n",
    "                            'regularization': 0.001\n",
    "                        }\n",
    "                        print(\"No valid parameters found for Seasonality. Using internal default parameters.\")\n",
    "                        \n",
    "                        # Use internal parameters for model creation\n",
    "                        best_params = internal_params\n",
    "\n",
    "                # Create callbacks\n",
    "                callbacks = create_callbacks(model_dir, component)\n",
    "\n",
    "                # Add ModelCheckpoint to save the best model\n",
    "                checkpoint_path = f\"{model_dir}/best_model_{component}.keras\"\n",
    "                model_checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "                callbacks.append(model_checkpoint)\n",
    "\n",
    "                # Train final model with best parameters\n",
    "                print(\"\\nTraining final model with best parameters...\")\n",
    "                final_model = create_optimized_model(**best_params)\n",
    "\n",
    "                # Train model\n",
    "                history = final_model.fit(train_generator, validation_data=test_generator, epochs=N_EPOCHS, steps_per_epoch=STEPS_PER_EPOCH, shuffle=False, callbacks=callbacks, verbose=1)\n",
    "\n",
    "                # Save the best hyperparameters to a file\n",
    "                with open(hyperparams_file, \"w\") as f:\n",
    "                    f.write(\"Best Hyperparameters:\\n\")\n",
    "                    for param, value in best_params.items():\n",
    "                        f.write(f\"{param}: {value}\\n\")\n",
    "\n",
    "                print(f\"Best hyperparameters saved to {hyperparams_file}\")\n",
    "\n",
    "\n",
    "\n",
    "                # Generate predictions\n",
    "                print(\"Generating predictions...\")\n",
    "                prediction = final_model.predict(test_generator)\n",
    "                pre_train = final_model.predict(train_generator)\n",
    "                \n",
    "                # Process predictions\n",
    "                rescaled_prediction_test, rescaled_prediction_train = process_predictions(prediction, pre_train, test, train, N_INPUT, scaler)\n",
    "                \n",
    "                # Save predictions\n",
    "                # test_pr, train_pr = save_predictions(rescaled_prediction_test,rescaled_prediction_train,component,model_dir)\n",
    "                # Example call to save_predictions\n",
    "                # test_pr, train_pr = save_predictions(rescaled_prediction_test, rescaled_prediction_train, component, model_dir, test, train)\n",
    "                # Example call to save_predictions within your training loop\n",
    "                try:\n",
    "                    # Assuming rescaled_prediction_test and rescaled_prediction_train are defined\n",
    "                    test_pr, train_pr = save_predictions(rescaled_prediction_test, rescaled_prediction_train, component, model_dir)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {component} component: {str(e)}\")\n",
    "                # Before calling save_predictions\n",
    "                print(f\"Shapes - Test Predictions: {rescaled_prediction_test.shape}, Train Predictions: {rescaled_prediction_train.shape}\")\n",
    "\n",
    "                # Call save_predictions\n",
    "                test_pr, train_pr = save_predictions(rescaled_prediction_test, rescaled_prediction_train, component, model_dir)\n",
    "\n",
    "\n",
    "                # Save model history\n",
    "                save_model_history(history, model_dir, component)\n",
    "                \n",
    "                # Store component results\n",
    "                dataset_results['components'][component] = {'best_params': best_params,'best_val_loss': best_val_loss,'final_val_loss': history.history['val_loss'][-1],'final_val_mae': history.history['val_mae'][-1]}\n",
    "                \n",
    "                print(f\"Completed {component} component\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {component} component: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Save dataset results summary\n",
    "        results_df = pd.DataFrame.from_dict(\n",
    "            {(i,j): dataset_results['components'][i][j] \n",
    "             for i in dataset_results['components'].keys() \n",
    "             for j in dataset_results['components'][i].keys()},\n",
    "            orient='index'\n",
    "        )\n",
    "        results_df.to_csv(f'{model_dir}/model_summary.csv')\n",
    "        \n",
    "        # Create summary plots\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Plot final validation losses\n",
    "        plt.subplot(1, 2, 1)\n",
    "        val_losses = [data['final_val_loss'] \n",
    "                     for data in dataset_results['components'].values()]\n",
    "        plt.bar(components, val_losses)\n",
    "        plt.title('Final Validation Loss by Component')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.ylabel('Loss')\n",
    "        \n",
    "        # Plot final MAE\n",
    "        plt.subplot(1, 2, 2)\n",
    "        val_maes = [data['final_val_mae'] \n",
    "                   for data in dataset_results['components'].values()]\n",
    "        plt.bar(components, val_maes)\n",
    "        plt.title('Final Validation MAE by Component')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.ylabel('MAE')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{model_dir}/final_metrics_summary.png')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Completed dataset {ds+1}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing dataset {ds}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\nTraining completed for all datasets!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e86cc302-17d6-4a13-bc23-da6f99ef2f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename ='AO_BiGru'\n",
    "Pre_Ds='Res_Data/'\n",
    "vip=f'jupyter nbconvert --to html {filename}.ipynb --stdout > {Pre_Ds}{MODEL_NAME}.html'\n",
    "os.system(vip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "59df719f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n    var kernel = IPython.notebook.kernel;\n    kernel.execute('notebook_name = \"' + IPython.notebook.notebook_name + '\"');\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\n    var kernel = IPython.notebook.kernel;\n    kernel.execute('notebook_path = \"' + IPython.notebook.notebook_path + '\"');\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                 Type                   Data/Info\n",
      "---------------------------------------------------------\n",
      "Adam                     type                   <class 'keras.src.optimizers.adam.Adam'>\n",
      "BATCH_SIZE               int                    1\n",
      "Back                     AnsiBack               <colorama.ansi.AnsiBack o<...>ct at 0x000002C68EEF0F10>\n",
      "Bidirectional            type                   <class 'keras.src.layers.<...>rectional.Bidirectional'>\n",
      "Dense                    type                   <class 'keras.src.layers.core.dense.Dense'>\n",
      "Dropout                  type                   <class 'keras.src.layers.<...>ization.dropout.Dropout'>\n",
      "EarlyStopping            type                   <class 'keras.src.callbac<...>_stopping.EarlyStopping'>\n",
      "FORES                    list                   n=7\n",
      "Fore                     AnsiFore               <colorama.ansi.AnsiFore o<...>ct at 0x000002C68EEF0E80>\n",
      "GRU                      type                   <class 'keras.src.layers.rnn.gru.GRU'>\n",
      "Input                    function               <function Input at 0x000002C6DCF2BA30>\n",
      "Javascript               type                   <class 'IPython.core.display.Javascript'>\n",
      "K                        module                 <module 'keras.backend' f<...>i\\\\backend\\\\__init__.py'>\n",
      "Layer                    type                   <class 'keras.src.layers.layer.Layer'>\n",
      "MODEL_NAME               str                    BiGru_Optimized\n",
      "MinMaxScaler             type                   <class 'sklearn.preproces<...>sing._data.MinMaxScaler'>\n",
      "ModelCheckpoint          type                   <class 'keras.src.callbac<...>ckpoint.ModelCheckpoint'>\n",
      "MyCustomDataset          type                   <class '__main__.MyCustomDataset'>\n",
      "N_EPOCHS                 int                    300\n",
      "N_FEATURES               int                    1\n",
      "N_INPUT                  int                    24\n",
      "Pre_Ds                   str                    Res_Data\\\n",
      "RMSprop                  type                   <class 'keras.src.optimizers.rmsprop.RMSprop'>\n",
      "ReduceLROnPlateau        type                   <class 'keras.src.callbac<...>ateau.ReduceLROnPlateau'>\n",
      "RobustScaler             type                   <class 'sklearn.preproces<...>sing._data.RobustScaler'>\n",
      "SGD                      type                   <class 'keras.src.optimizers.sgd.SGD'>\n",
      "STEPS_PER_EPOCH          int                    320\n",
      "Sequence                 type                   <class 'keras.src.trainer<...>taset_adapter.PyDataset'>\n",
      "Sequential               type                   <class 'keras.src.models.sequential.Sequential'>\n",
      "StandardScaler           type                   <class 'sklearn.preproces<...>ng._data.StandardScaler'>\n",
      "Style                    AnsiStyle              <colorama.ansi.AnsiStyle <...>ct at 0x000002C68EEF0F70>\n",
      "TensorBoard              type                   <class 'keras.src.callbac<...>tensorboard.TensorBoard'>\n",
      "TimeseriesGenerator      type                   <class 'keras.src.legacy.<...>nce.TimeseriesGenerator'>\n",
      "attention                type                   <class '__main__.attention'>\n",
      "component                str                    Original\n",
      "components               list                   n=4\n",
      "create_callbacks         function               <function create_callbacks at 0x000002C6F06BCA60>\n",
      "create_optimized_model   function               <function create_optimize<...>el at 0x000002C6DE2CFE20>\n",
      "dataset_results          dict                   n=2\n",
      "dataso                   str                    Res_Data\\WEST_BENGAL\\DURGAPUR\n",
      "df                       DataFrame                                   DATE<...>n[17434 rows x 2 columns]\n",
      "df1                      DataFrame                                   Seas<...>n[17410 rows x 4 columns]\n",
      "dh                       list                   n=17\n",
      "display_javascript       function               <function display_javascr<...>pt at 0x000002C69044B0A0>\n",
      "ds                       int                    16\n",
      "file_name                str                    ../DataSets\\WEST_BENGAL\\D<...>_ (copy)_INDEX_Median.csv\n",
      "filename                 str                    AO_BiGru\n",
      "get_notebook_name        function               <function get_notebook_na<...>me at 0x000002C6F29EAE60>\n",
      "get_notebook_path        function               <function get_notebook_pa<...>th at 0x000002C6DEC79AB0>\n",
      "glob                     module                 <module 'glob' from 'C:\\\\<...>n2kfra8p0\\\\lib\\\\glob.py'>\n",
      "init                     function               <function init at 0x000002C68EF1A830>\n",
      "l1                       type                   <class 'keras.src.regularizers.regularizers.L1'>\n",
      "l2                       type                   <class 'keras.src.regularizers.regularizers.L2'>\n",
      "li                       list                   n=17\n",
      "model_configs            dict                   n=5\n",
      "model_dir                str                    Res_Data\\WEST_BENGAL\\DURGAPUR/BiGru_Optimized\n",
      "notebook_name            NoneType               None\n",
      "notebook_path            NoneType               None\n",
      "np                       module                 <module 'numpy' from 'C:\\<...>ges\\\\numpy\\\\__init__.py'>\n",
      "os                       module                 <module 'os' from 'C:\\\\Pr<...>z5n2kfra8p0\\\\lib\\\\os.py'>\n",
      "pd                       module                 <module 'pandas' from 'C:<...>es\\\\pandas\\\\__init__.py'>\n",
      "plt                      module                 <module 'matplotlib.pyplo<...>\\\\matplotlib\\\\pyplot.py'>\n",
      "prepare_component_data   function               <function prepare_compone<...>ta at 0x000002C6F06BC040>\n",
      "process_dataset          function               <function process_dataset at 0x000002C6E1B871C0>\n",
      "process_predictions      function               <function process_predict<...>ns at 0x000002C6E230C8B0>\n",
      "results_df               DataFrame              Empty DataFrame\\nColumns: []\\nIndex: []\n",
      "save_model_history       function               <function save_model_hist<...>ry at 0x000002C6925E40D0>\n",
      "save_predictions         function               <function save_predictions at 0x000002C6F298FD00>\n",
      "scaler                   MinMaxScaler           MinMaxScaler()\n",
      "sessaon_name             NoneType               None\n",
      "session_name             NoneType               None\n",
      "simple_grid_search       function               <function simple_grid_sea<...>ch at 0x000002C6DE2CFD90>\n",
      "sm                       module                 <module 'statsmodels.api'<...>es\\\\statsmodels\\\\api.py'>\n",
      "test                     ndarray                5223x1: 5223 elems, type `float64`, 41784 bytes\n",
      "test_generator           TimeseriesGenerator    <keras.src.legacy.preproc<...>ct at 0x000002C6E1857AF0>\n",
      "tf                       module                 <module 'tensorflow' from<...>tensorflow\\\\__init__.py'>\n",
      "time                     module                 <module 'time' (built-in)>\n",
      "train                    ndarray                12187x1: 12187 elems, type `float64`, 97496 bytes\n",
      "train_generator          TimeseriesGenerator    <keras.src.legacy.preproc<...>ct at 0x000002C6E1F4DE40>\n",
      "train_test_split         function               <function train_test_split at 0x000002C6D5059750>\n",
      "val_losses               list                   n=0\n",
      "vip                      str                    jupyter nbconvert --to ht<...>Data\\BiGru_Optimized.html\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import Javascript, display_javascript\n",
    "\n",
    "def get_notebook_name():\n",
    "    \"\"\"Execute JS code to save Jupyter notebook name to variable notebook_name\"\"\"\n",
    "    js = Javascript(\"\"\"\n",
    "    var kernel = IPython.notebook.kernel;\n",
    "    kernel.execute('notebook_name = \"' + IPython.notebook.notebook_name + '\"');\n",
    "    \"\"\")\n",
    "    return display_javascript(js)\n",
    "\n",
    "def get_notebook_path():\n",
    "    \"\"\"Execute JS code to save Jupyter notebook path to variable notebook_path\"\"\"\n",
    "    js = Javascript(\"\"\"\n",
    "    var kernel = IPython.notebook.kernel;\n",
    "    kernel.execute('notebook_path = \"' + IPython.notebook.notebook_path + '\"');\n",
    "    \"\"\")\n",
    "    return display_javascript(js)\n",
    "notebook_name = get_notebook_name()\n",
    "notebook_path = get_notebook_path()\n",
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "639e4924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(f\"{notebook_name}\")\n",
    "print(f\"{notebook_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bb3d4620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(f\"{notebook_name}\")\n",
    "print(f\"{notebook_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "12537e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessaon_name = os.getenv('JPY_SESSION_NAME')\n",
    "sessaon_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5a40530d",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_name = os.getenv('JPY_SESSION_NAME')\n",
    "session_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887460ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
