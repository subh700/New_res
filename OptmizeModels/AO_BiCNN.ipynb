{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "578bb39b-1c63-455f-bc5a-8d34df28811d",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc127028-217c-443f-aeca-1c2d70986fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Imports and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "import statsmodels.api as sm\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import keras.backend as K\n",
    "from keras.layers import Layer\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "from colorama import init, Fore, Back, Style\n",
    "from keras.regularizers import l1, l2\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, SGD\n",
    "from tensorflow.keras.layers import (Input, Conv1D, Bidirectional, BatchNormalization, \n",
    "                                     Dropout, Flatten, Dense, Lambda, MaxPooling1D, \n",
    "                                     Concatenate)\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d1ff166-9bb5-425a-b0be-a68e0d14d99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available foreground colors for logging\n",
    "FORES = [Fore.RED, Fore.GREEN, Fore.YELLOW, Fore.BLUE, Fore.MAGENTA, Fore.CYAN, Fore.WHITE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "551c7893-5ae1-4285-bdb9-918c53d45901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter configurations\n",
    "# model_configs = {'units_1': [128],'units_2': [32],'dropout_rate': [0.1],'learning_rate': [0.001],'regularization': [0.001]}\n",
    "model_configs = {'units_1': [64, 128],'units_2': [32, 64],'dropout_rate': [0.1, 0.2],'learning_rate': [0.001, 0.01],'regularization': [0.001, 0.01]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a0ddb07-7205-49d2-b10f-33a263648d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class MyCustomDataset(Sequence):\n",
    "    def __init__(self, data, labels, batch_size=32, **kwargs):\n",
    "        super().__init__(**kwargs)  # Call the parent constructor with kwargs\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.indexes = np.arange(len(self.data))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Generate one batch of data\n",
    "        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_data = self.data[batch_indexes]\n",
    "        batch_labels = self.labels[batch_indexes]\n",
    "        return batch_data, batch_labels\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Shuffle indexes after each epoch if needed\n",
    "        np.random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33097b7b-b13d-45f8-9bb3-3bb5379dc6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "MODEL_NAME = 'BiCNN_Optimized'\n",
    "N_EPOCHS = 300\n",
    "STEPS_PER_EPOCH = 320\n",
    "N_INPUT = 24\n",
    "N_FEATURES = 1\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ffa1cdb-e57b-43cc-a03b-81515bbca2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17 data files\n"
     ]
    }
   ],
   "source": [
    "# Load data paths\n",
    "dh = glob.glob('../DataSets/*/*/H*_INDEX_*.csv')\n",
    "print(f\"Found {len(dh)} data files\")\n",
    "\n",
    "# Create initial data list\n",
    "li = []\n",
    "for file_name in dh:\n",
    "    df = pd.read_csv(file_name)\n",
    "    li.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad1a94b-bf7a-4366-9e35-af91c0df3174",
   "metadata": {},
   "source": [
    "# Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "856998bb-070b-4a8f-8bce-d62fc71b709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Data Processing Functions\n",
    "\n",
    "def process_dataset(df, data_path):\n",
    "    \"\"\"Process a single dataset with proper path handling\"\"\"\n",
    "    # Create output directory path\n",
    "    dataso = 'Res_Data/' + data_path.split('/')[2] + '/' + data_path.split('/')[3]\n",
    "    \n",
    "    # Convert date and set index\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    df = df.set_index('DATE')\n",
    "    \n",
    "    # Decompose time series\n",
    "    decomposition = sm.tsa.seasonal_decompose(df['PM2.5'], model='additive')\n",
    "    \n",
    "    # Create DataFrame with components\n",
    "    df1 = pd.DataFrame()\n",
    "    df1['Date'] = df.index\n",
    "    df1['Seasonality'] = decomposition.seasonal.values\n",
    "    df1['Trend'] = decomposition.trend.values\n",
    "    df1['Noise'] = decomposition.resid.values\n",
    "    df1['Original'] = df.iloc[:,-1:].values\n",
    "    \n",
    "    # Process dataframe\n",
    "    df1 = df1.iloc[12:-12,:]\n",
    "    df1 = df1.reset_index()\n",
    "    df1 = df1.iloc[:,1:]\n",
    "    df1 = df1.set_index('Date')\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(dataso, exist_ok=True)\n",
    "    \n",
    "    # Save decomposed data\n",
    "    df1.to_csv(dataso + '/' + data_path.split('/')[4])\n",
    "    \n",
    "    return df1, dataso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "488d60c2-6311-4b27-972d-9287271dd6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_component_data(df, component_name, dataso):\n",
    "    \"\"\"Prepare data for a specific component, checking for existing CSV files.\"\"\"\n",
    "    # Define the path for the component CSV file\n",
    "    component_file_path = os.path.join(dataso, f'{component_name}.csv')\n",
    "    \n",
    "    # Check if the component CSV file already exists\n",
    "    if os.path.exists(component_file_path):\n",
    "        print(f\"Loading existing data for {component_name} from {component_file_path}\")\n",
    "        component_df = pd.read_csv(component_file_path, index_col=0)\n",
    "    else:\n",
    "        print(f\"Creating new data for {component_name}...\")\n",
    "        # Select appropriate component data\n",
    "        if component_name == 'Seasonality':\n",
    "            component_df = df.iloc[:, :1]\n",
    "        elif component_name == 'Trend':\n",
    "            component_df = df.iloc[:, 1:2]\n",
    "        elif component_name == 'Noise':\n",
    "            component_df = df.iloc[:, 2:3]\n",
    "        else:  # Original\n",
    "            component_df = df.iloc[:, 3:4]\n",
    "        \n",
    "        # Save component data\n",
    "        component_df.to_csv(component_file_path)\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled = scaler.fit_transform(component_df.values)\n",
    "    \n",
    "    # Split data\n",
    "    train, test = train_test_split(scaled, test_size=0.30, shuffle=False)\n",
    "    train_target = train[:]\n",
    "    test_target = test[:]\n",
    "    \n",
    "    # Create generators\n",
    "    train_generator = TimeseriesGenerator(\n",
    "        train, train_target, \n",
    "        length=N_INPUT, \n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "    test_generator = TimeseriesGenerator(\n",
    "        test, test_target, \n",
    "        length=N_INPUT, \n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    return train_generator, test_generator, scaler, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bc0e1e6-5a49-4da1-bb9b-7bcabba240fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Attention Layer\n",
    "class attention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(attention,self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self,input_shape):\n",
    "        self.W = self.add_weight(name='attention_weight', shape=(input_shape[-1],1), initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(name='attention_bias', shape=(input_shape[1],1), initializer='zeros', trainable=True)        \n",
    "        super(attention, self).build(input_shape)\n",
    "    \n",
    "    def call(self,x):\n",
    "        # Alignment scores. Pass them through tanh function\n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "        # Remove dimension of size 1\n",
    "        e = K.squeeze(e, axis=-1)   \n",
    "        # Compute the weights\n",
    "        alpha = K.softmax(e)\n",
    "        # Reshape to tensorFlow format\n",
    "        alpha = K.expand_dims(alpha, axis=-1)\n",
    "        # Compute the context vector\n",
    "        context = x * alpha\n",
    "        context = K.sum(context, axis=1)\n",
    "        return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b22e1021-1524-4a17-8133-f1bf98ae51cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def simple_grid_search(train_generator, test_generator, configs):\n",
    "#     \"\"\"Perform manual grid search for hyperparameters\"\"\"\n",
    "#     best_val_loss = float('inf')\n",
    "#     best_params = None\n",
    "#     best_model = None\n",
    "    \n",
    "#     # Try different combinations\n",
    "#     for units_1 in configs['units_1']:\n",
    "#         for units_2 in configs['units_2']:\n",
    "#             for dropout in configs['dropout_rate']:\n",
    "#                 for lr in configs['learning_rate']:\n",
    "#                     for reg in configs['regularization']:\n",
    "#                         print(f\"\\nTrying parameters: units1={units_1}, units2={units_2}, \" f\"dropout={dropout}, lr={lr}, reg={reg}\")\n",
    "                        \n",
    "#                         # Create and train model\n",
    "#                         model = create_optimized_model(units_1=units_1,units_2=units_2,dropout_rate=dropout,learning_rate=lr,regularization=reg)\n",
    "                        \n",
    "#                         # Early stopping\n",
    "#                         es = EarlyStopping(monitor='val_loss', mode='min', patience=5, verbose=0)\n",
    "                        \n",
    "#                         # Train for fewer epochs during search\n",
    "#                         history = model.fit(train_generator,validation_data=test_generator,epochs=50, steps_per_epoch=STEPS_PER_EPOCH,shuffle=False,callbacks=[es],verbose=0)\n",
    "                        \n",
    "#                         val_loss = min(history.history['val_loss'])\n",
    "                        \n",
    "#                         if val_loss < best_val_loss:\n",
    "#                             best_val_loss = val_loss\n",
    "#                             best_params = {'units_1': units_1,'units_2': units_2,'dropout_rate': dropout,'learning_rate': lr,'regularization': reg}\n",
    "#                             best_model = model\n",
    "                            \n",
    "#                         print(f\"Val Loss: {val_loss:.4f}\")\n",
    "                        \n",
    "#     return best_params, best_model, best_val_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def simple_grid_search(train_generator, test_generator, configs, model_dir, component):\n",
    "    \"\"\"Perform manual grid search for hyperparameters\"\"\"\n",
    "    best_val_loss = float('inf')\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    # Check if a model already exists\n",
    "    model_path = f'{model_dir}/best_model_{component}.keras'\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Loading existing model for {component} from {model_path}\")\n",
    "        best_model = tf.keras.models.load_model(model_path)\n",
    "        # Return None for best_params since we are not performing a search\n",
    "        return None, best_model, best_val_loss\n",
    "    \n",
    "    # Try different combinations\n",
    "    for units_1 in configs['units_1']:\n",
    "        for units_2 in configs['units_2']:\n",
    "            for dropout in configs['dropout_rate']:\n",
    "                for lr in configs['learning_rate']:\n",
    "                    for reg in configs['regularization']:\n",
    "                        print(f\"\\nTrying parameters: units1={units_1}, units2={units_2}, \" f\"dropout={dropout}, lr={lr}, reg={reg}\")\n",
    "                        \n",
    "                        # Create and train model\n",
    "                        model = create_optimized_model(units_1=units_1,units_2=units_2,dropout_rate=dropout,learning_rate=lr,regularization=reg)\n",
    "                        \n",
    "                        # Early stopping\n",
    "                        es = EarlyStopping(monitor='val_loss', mode='min', patience=5, verbose=0)\n",
    "                        \n",
    "                        # Train for fewer epochs during search\n",
    "                        history = model.fit(train_generator, validation_data=test_generator, epochs=50, steps_per_epoch=STEPS_PER_EPOCH, shuffle=False, callbacks=[es], verbose=0)\n",
    "                        \n",
    "                        val_loss = min(history.history['val_loss'])\n",
    "                        \n",
    "                        if val_loss < best_val_loss:\n",
    "                            best_val_loss = val_loss\n",
    "                            best_params = {'units_1': units_1, 'units_2': units_2, 'dropout_rate': dropout, 'learning_rate': lr, 'regularization': reg}\n",
    "                            best_model = model\n",
    "                            \n",
    "                        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # print(f\"Best parameters: {best_params}, Best validation loss: {best_val_loss:.4f}\")  # Debugging line\n",
    "    print(f\"Best parameters: {best_params}, Best validation loss: {best_val_loss:.4f}\")  # Debugging line\n",
    "    return best_params, best_model, best_val_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9b7def-bb24-4425-9178-4be12829eaf0",
   "metadata": {},
   "source": [
    "# Model Definition and Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5cf0bef-7ed2-4fc3-ba3f-868ea5a11b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Model Definition and Architecture\n",
    "\n",
    "def create_optimized_model(units_1=256, units_2=128, dropout_rate=0.3, \n",
    "                         learning_rate=0.001, regularization=0.002):\n",
    "    \"\"\"Create the optimized Bidirectional CNN model with specified hyperparameters\"\"\"\n",
    "    \n",
    "    # Create model using Functional API\n",
    "    input_shape = (N_INPUT, N_FEATURES)\n",
    "    \n",
    "    # Define the input layer explicitly\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Forward path\n",
    "    forward = Conv1D(filters=units_1, kernel_size=3, activation='relu', padding='same', \n",
    "                     kernel_regularizer=l1(regularization), bias_regularizer=l1(0.4))(inputs)\n",
    "    forward = BatchNormalization()(forward)\n",
    "    forward = Dropout(dropout_rate)(forward)\n",
    "    \n",
    "    forward = Conv1D(filters=units_2, kernel_size=3, activation='relu', padding='same', \n",
    "                     bias_regularizer=l1(0.2))(forward)\n",
    "    forward = BatchNormalization()(forward)\n",
    "    forward = Dropout(dropout_rate)(forward)\n",
    "    \n",
    "    # Backward path (using reversed input)\n",
    "    backward = Lambda(lambda x: tf.reverse(x, axis=[1]), output_shape=(N_INPUT, N_FEATURES))(inputs)\n",
    "    backward = Conv1D(filters=units_1, kernel_size=3, activation='relu', padding='same', \n",
    "                       kernel_regularizer=l1(regularization), bias_regularizer=l1(0.4))(backward)\n",
    "    backward = BatchNormalization()(backward)\n",
    "    backward = Dropout(dropout_rate)(backward)\n",
    "    \n",
    "    backward = Conv1D(filters=units_2, kernel_size=3, activation='relu', padding='same', \n",
    "                       bias_regularizer=l1(0.2))(backward)\n",
    "    backward = BatchNormalization()(backward)\n",
    "    backward = Dropout(dropout_rate)(backward)\n",
    "    \n",
    "    # Merge forward and backward paths\n",
    "    merged = Concatenate(axis=2)([forward, backward])\n",
    "    \n",
    "    # Shared layers after merging\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', \n",
    "                bias_regularizer=l1(0.2))(merged)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    # Dense layers\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Compile model\n",
    "    optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f59e3ec8-dc82-45fb-a251-dc834b325480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_callbacks(model_dir, component):\n",
    "    \"\"\"Create callbacks for model training\"\"\"\n",
    "    callbacks = [\n",
    "        # Early Stopping\n",
    "        EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=12),\n",
    "        \n",
    "        # Model Checkpoint\n",
    "        ModelCheckpoint(filepath=f'{model_dir}/best_model_{component}.keras',monitor='val_loss',mode='min',save_best_only=True,verbose=1),\n",
    "        \n",
    "        # Reduce Learning Rate\n",
    "        ReduceLROnPlateau(monitor='val_loss',factor=0.2,patience=5,min_lr=0.0001,verbose=1)\n",
    "    ]\n",
    "    \n",
    "    return callbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "941b5ac6-877b-4a42-8f86-4b6daf4b0354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_predictions(prediction, pre_train, test, train, n_input, scaler):\n",
    "    \"\"\"Process model predictions\"\"\"\n",
    "    # Process test predictions\n",
    "    test1 = test[n_input:,:]\n",
    "    prediction1 = np.concatenate((prediction, test1), axis=1) if prediction.size > 0 and test1.size > 0 else np.array([])\n",
    "    rescaled_prediction = scaler.inverse_transform(prediction1) if prediction1.size > 0 else np.array([])\n",
    "    rescaled_prediction_test = rescaled_prediction[:,0] if rescaled_prediction.size > 0 else np.array([])\n",
    "    \n",
    "    # Process train predictions\n",
    "    train1 = train[n_input:,:]\n",
    "    pre_train1 = np.concatenate((pre_train, train1), axis=1) if pre_train.size > 0 and train1.size > 0 else np.array([])\n",
    "    rescaled_prediction_trai = scaler.inverse_transform(pre_train1) if pre_train1.size > 0 else np.array([])\n",
    "    rescaled_prediction_train = rescaled_prediction_trai[:,0] if rescaled_prediction_trai.size > 0 else np.array([])\n",
    "    \n",
    "    return rescaled_prediction_test, rescaled_prediction_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "620d4ea2-831b-4741-ae46-7668d20bf497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(rescaled_prediction_test, rescaled_prediction_train, \n",
    "                     component, model_dir):\n",
    "    \"\"\"Save model predictions\"\"\"\n",
    "    # Create DataFrames for predictions\n",
    "    if component == 'Original':\n",
    "        test_pr = pd.DataFrame(rescaled_prediction_test, columns=['Original_Predicted']) if rescaled_prediction_test.size > 0 else pd.DataFrame()\n",
    "        train_pr = pd.DataFrame(rescaled_prediction_train, columns=['Original_Predicted']) if rescaled_prediction_train.size > 0 else pd.DataFrame()\n",
    "    else:\n",
    "        # Load existing predictions if available\n",
    "        test_pr = pd.read_csv(f'{model_dir}/test_pre.csv', index_col=0) \\\n",
    "                 if os.path.exists(f'{model_dir}/test_pre.csv') \\\n",
    "                 else pd.DataFrame()\n",
    "        train_pr = pd.read_csv(f'{model_dir}/train_pre.csv', index_col=0) \\\n",
    "                  if os.path.exists(f'{model_dir}/train_pre.csv') \\\n",
    "                  else pd.DataFrame()\n",
    "        \n",
    "        # Add new predictions if they are not empty\n",
    "        if rescaled_prediction_test.size > 0:\n",
    "            test_pr[f'{component}_Predicted'] = rescaled_prediction_test\n",
    "        if rescaled_prediction_train.size > 0:\n",
    "            train_pr[f'{component}_Predicted'] = rescaled_prediction_train\n",
    "    \n",
    "    # Save to files with component-specific names\n",
    "    test_pr.to_csv(f'{model_dir}/{component}_test_predictions.csv', index=False)\n",
    "    train_pr.to_csv(f'{model_dir}/{component}_train_predictions.csv', index=False)\n",
    "    \n",
    "    return test_pr, train_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b12973a2-2ba9-4beb-a1f6-fec77207365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_history(history, model_dir, component):\n",
    "    \"\"\"Save model training history\"\"\"\n",
    "    hist_df = pd.DataFrame(history.history)\n",
    "    hist_df.to_csv(f'{model_dir}/{component}_history.csv')\n",
    "    \n",
    "    # Create and save training plots\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{component} Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot MAE\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='Training MAE')\n",
    "    plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "    plt.title(f'{component} Model MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{model_dir}/{component}_training_history.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebe56f2-95b5-4b80-8ddb-1f89bb9edae4",
   "metadata": {},
   "source": [
    "# Training and Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6996b4b8-2ac8-4d70-9dd0-24edee2fb4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset 1/17\n",
      "\u001b[31mDATASET----------------../DataSets\\GUJARAT\\ANKLESHWAR\\H_Ankl_1_2_19-3_12_22-_41_ (copy)_INDEX_Mean.csv\n",
      "\n",
      "Processing Seasonality component...\n",
      "Creating new data for Seasonality...\n",
      "Data shapes - Train: (23457, 1), Test: (10054, 1)\n",
      "Performing hyperparameter search...\n",
      "\n",
      "Trying parameters: units1=64, units2=32, dropout=0.1, lr=0.001, reg=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vp123\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 71\u001b[0m\n\u001b[0;32m     68\u001b[0m             best_params[param] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(value) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m value \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mint\u001b[39m(value)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# Perform hyperparameter search if no file exists\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m     best_params, best_model, best_val_loss \u001b[38;5;241m=\u001b[39m \u001b[43msimple_grid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_configs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomponent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;66;03m# Check if best_params is None before proceeding\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m best_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[10], line 68\u001b[0m, in \u001b[0;36msimple_grid_search\u001b[1;34m(train_generator, test_generator, configs, model_dir, component)\u001b[0m\n\u001b[0;32m     65\u001b[0m es \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Train for fewer epochs during search\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSTEPS_PER_EPOCH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mes\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_loss \u001b[38;5;241m<\u001b[39m best_val_loss:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:392\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_epoch_iterator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_epoch_iterator \u001b[38;5;241m=\u001b[39m TFEpochIterator(\n\u001b[0;32m    383\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m    384\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    391\u001b[0m     )\n\u001b[1;32m--> 392\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    402\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    404\u001b[0m }\n\u001b[0;32m    405\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:481\u001b[0m, in \u001b[0;36mTensorFlowTrainer.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    480\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m--> 481\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    482\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_end(step, logs)\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_evaluating:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:216\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    214\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    218\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1698\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Section 4: Training and Evaluation Loop\n",
    "\n",
    "# Define components to process\n",
    "components = ['Seasonality', 'Trend', 'Noise', 'Original']\n",
    "\n",
    "# Main training loop\n",
    "for ds in range(len(dh)):\n",
    "    print(f\"\\nProcessing dataset {ds+1}/{len(dh)}\")\n",
    "    print(FORES[ds % len(FORES)] + f'DATASET----------------{dh[ds]}')\n",
    "    \n",
    "    try:\n",
    "        # Process dataset\n",
    "        df1, dataso = process_dataset(li[ds], dh[ds])\n",
    "        \n",
    "        # Create model directory\n",
    "        model_dir = dataso + '/' + MODEL_NAME\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "        # Store results for this dataset\n",
    "        dataset_results = {\n",
    "            'dataset': dh[ds],\n",
    "            'components': {}\n",
    "        }\n",
    "        \n",
    "        # Process each component\n",
    "        for component in components:\n",
    "            print(f\"\\nProcessing {component} component...\")\n",
    "            \n",
    "            try:\n",
    "                # Prepare data\n",
    "                train_generator, test_generator, scaler, train, test = prepare_component_data(df1, component, dataso)\n",
    "                \n",
    "                print(f\"Data shapes - Train: {train.shape}, Test: {test.shape}\")\n",
    "                \n",
    "                # Perform grid search\n",
    "                print(\"Performing hyperparameter search...\")\n",
    "                # best_params, best_model, best_val_loss = simple_grid_search(train_generator, test_generator, model_configs)\n",
    "                \n",
    "                # print(\"\\nBest parameters found:\")\n",
    "                # for param, value in best_params.items():\n",
    "                #     print(f\"{param}: {value}\")\n",
    "                # print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "                \n",
    "                # # Create callbacks\n",
    "                # callbacks = create_callbacks(model_dir, component)\n",
    "                \n",
    "                # # Train final model with best parameters\n",
    "                # print(\"\\nTraining final model with best parameters...\")\n",
    "                # final_model = create_optimized_model(**best_params)\n",
    "                \n",
    "                # # Train model\n",
    "                # history = final_model.fit(train_generator,validation_data=test_generator,epochs=N_EPOCHS,steps_per_epoch=STEPS_PER_EPOCH,shuffle=False,callbacks=callbacks,verbose=1)\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "                # Define the path for the best hyperparameters file\n",
    "                hyperparams_file = f\"{model_dir}/best_hyperparameters_{component}.txt\"\n",
    "\n",
    "                # Check if the best hyperparameters file exists\n",
    "                if os.path.exists(hyperparams_file):\n",
    "                    print(f\"Loading existing best hyperparameters for {component} from {hyperparams_file}\")\n",
    "                    with open(hyperparams_file, \"r\") as f:\n",
    "                        # Read the hyperparameters from the file\n",
    "                        best_params = {}\n",
    "                        for line in f.readlines()[1:]:  # Skip the first line\n",
    "                            param, value = line.strip().split(\": \")\n",
    "                            best_params[param] = float(value) if '.' in value else int(value)\n",
    "                else:\n",
    "                    # Perform hyperparameter search if no file exists\n",
    "                    best_params, best_model, best_val_loss = simple_grid_search(train_generator, test_generator, model_configs, model_dir, component)\n",
    "\n",
    "                    # Check if best_params is None before proceeding\n",
    "                    if best_params is not None:\n",
    "                        print(\"\\nBest parameters found:\")\n",
    "                        for param, value in best_params.items():\n",
    "                            print(f\"{param}: {value}\")\n",
    "                        print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "                    else:\n",
    "                        # Set internal default parameters if no valid parameters were found\n",
    "                        internal_params = {\n",
    "                            'units_1': 64,\n",
    "                            'units_2': 32,\n",
    "                            'dropout_rate': 0.1,\n",
    "                            'learning_rate': 0.001,\n",
    "                            'regularization': 0.001\n",
    "                        }\n",
    "                        print(\"No valid parameters found for Seasonality. Using internal default parameters.\")\n",
    "                        \n",
    "                        # Use internal parameters for model creation\n",
    "                        best_params = internal_params\n",
    "\n",
    "                # Create callbacks\n",
    "                callbacks = create_callbacks(model_dir, component)\n",
    "\n",
    "                # Add ModelCheckpoint to save the best model\n",
    "                checkpoint_path = f\"{model_dir}/best_model_{component}.keras\"\n",
    "                model_checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "                callbacks.append(model_checkpoint)\n",
    "\n",
    "                # Train final model with best parameters\n",
    "                print(\"\\nTraining final model with best parameters...\")\n",
    "                final_model = create_optimized_model(**best_params)\n",
    "\n",
    "                # Train model\n",
    "                history = final_model.fit(train_generator, validation_data=test_generator, epochs=N_EPOCHS, steps_per_epoch=STEPS_PER_EPOCH, shuffle=False, callbacks=callbacks, verbose=1)\n",
    "\n",
    "                # Save the best hyperparameters to a file\n",
    "                with open(hyperparams_file, \"w\") as f:\n",
    "                    f.write(\"Best Hyperparameters:\\n\")\n",
    "                    for param, value in best_params.items():\n",
    "                        f.write(f\"{param}: {value}\\n\")\n",
    "\n",
    "                print(f\"Best hyperparameters saved to {hyperparams_file}\")\n",
    "\n",
    "\n",
    "\n",
    "                # Generate predictions\n",
    "                print(\"Generating predictions...\")\n",
    "                prediction = final_model.predict(test_generator)\n",
    "                pre_train = final_model.predict(train_generator)\n",
    "                \n",
    "                # Process predictions\n",
    "                rescaled_prediction_test, rescaled_prediction_train = process_predictions(prediction, pre_train, test, train, N_INPUT, scaler)\n",
    "                \n",
    "                # Save predictions\n",
    "                # test_pr, train_pr = save_predictions(rescaled_prediction_test,rescaled_prediction_train,component,model_dir)\n",
    "                # Example call to save_predictions\n",
    "                # test_pr, train_pr = save_predictions(rescaled_prediction_test, rescaled_prediction_train, component, model_dir, test, train)\n",
    "                # Example call to save_predictions within your training loop\n",
    "                try:\n",
    "                    # Assuming rescaled_prediction_test and rescaled_prediction_train are defined\n",
    "                    test_pr, train_pr = save_predictions(rescaled_prediction_test, rescaled_prediction_train, component, model_dir)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {component} component: {str(e)}\")\n",
    "                # Before calling save_predictions\n",
    "                print(f\"Shapes - Test Predictions: {rescaled_prediction_test.shape}, Train Predictions: {rescaled_prediction_train.shape}\")\n",
    "\n",
    "                # Call save_predictions\n",
    "                test_pr, train_pr = save_predictions(rescaled_prediction_test, rescaled_prediction_train, component, model_dir)\n",
    "\n",
    "\n",
    "                # Save model history\n",
    "                save_model_history(history, model_dir, component)\n",
    "                \n",
    "                # Store component results\n",
    "                dataset_results['components'][component] = {'best_params': best_params,'best_val_loss': best_val_loss,'final_val_loss': history.history['val_loss'][-1],'final_val_mae': history.history['val_mae'][-1]}\n",
    "                \n",
    "                print(f\"Completed {component} component\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {component} component: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Save dataset results summary\n",
    "        results_df = pd.DataFrame.from_dict(\n",
    "            {(i,j): dataset_results['components'][i][j] \n",
    "             for i in dataset_results['components'].keys() \n",
    "             for j in dataset_results['components'][i].keys()},\n",
    "            orient='index'\n",
    "        )\n",
    "        results_df.to_csv(f'{model_dir}/model_summary.csv')\n",
    "        \n",
    "        # Create summary plots\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Plot final validation losses\n",
    "        plt.subplot(1, 2, 1)\n",
    "        val_losses = [data['final_val_loss'] \n",
    "                     for data in dataset_results['components'].values()]\n",
    "        plt.bar(components, val_losses)\n",
    "        plt.title('Final Validation Loss by Component')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.ylabel('Loss')\n",
    "        \n",
    "        # Plot final MAE\n",
    "        plt.subplot(1, 2, 2)\n",
    "        val_maes = [data['final_val_mae'] \n",
    "                   for data in dataset_results['components'].values()]\n",
    "        plt.bar(components, val_maes)\n",
    "        plt.title('Final Validation MAE by Component')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.ylabel('MAE')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{model_dir}/final_metrics_summary.png')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Completed dataset {ds+1}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing dataset {ds}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\nTraining completed for all datasets!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86cc302-17d6-4a13-bc23-da6f99ef2f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename ='AO_BiCNN'\n",
    "Pre_Ds='Res_Data/'\n",
    "vip=f'jupyter nbconvert --to html {filename}.ipynb --stdout > {Pre_Ds}{MODEL_NAME}.html'\n",
    "os.system(vip)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
