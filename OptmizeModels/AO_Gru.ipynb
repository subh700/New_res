{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "578bb39b-1c63-455f-bc5a-8d34df28811d",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc127028-217c-443f-aeca-1c2d70986fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Imports and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "import statsmodels.api as sm\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import keras.backend as K\n",
    "from keras.layers import Layer\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "from colorama import init, Fore, Back, Style\n",
    "from keras.regularizers import l1, l2\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, SGD\n",
    "from tensorflow.keras.layers import ( Dense, Dropout, GRU, Input)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d1ff166-9bb5-425a-b0be-a68e0d14d99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available foreground colors for logging\n",
    "FORES = [Fore.RED, Fore.GREEN, Fore.YELLOW, Fore.BLUE, Fore.MAGENTA, Fore.CYAN, Fore.WHITE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "551c7893-5ae1-4285-bdb9-918c53d45901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter configurations\n",
    "# model_configs = {'units_1': [128],'units_2': [32],'dropout_rate': [0.1],'learning_rate': [0.001],'regularization': [0.001]}\n",
    "model_configs = {'units_1': [64, 128],'units_2': [32, 64],'dropout_rate': [0.1, 0.2],'learning_rate': [0.001, 0.01],'regularization': [0.001, 0.01]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a0ddb07-7205-49d2-b10f-33a263648d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class MyCustomDataset(Sequence):\n",
    "    def __init__(self, data, labels, batch_size=32, **kwargs):\n",
    "        super().__init__(**kwargs)  # Call the parent constructor with kwargs\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.indexes = np.arange(len(self.data))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Generate one batch of data\n",
    "        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_data = self.data[batch_indexes]\n",
    "        batch_labels = self.labels[batch_indexes]\n",
    "        return batch_data, batch_labels\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Shuffle indexes after each epoch if needed\n",
    "        np.random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33097b7b-b13d-45f8-9bb3-3bb5379dc6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "MODEL_NAME = 'Gru_Optimized'\n",
    "N_EPOCHS = 300\n",
    "STEPS_PER_EPOCH = 320\n",
    "N_INPUT = 24\n",
    "N_FEATURES = 1\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ffa1cdb-e57b-43cc-a03b-81515bbca2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17 data files\n"
     ]
    }
   ],
   "source": [
    "# Load data paths\n",
    "dh = glob.glob('../DataSets/*/*/H*_INDEX_*.csv')\n",
    "print(f\"Found {len(dh)} data files\")\n",
    "\n",
    "# Create initial data list\n",
    "li = []\n",
    "for file_name in dh:\n",
    "    df = pd.read_csv(file_name)\n",
    "    li.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad1a94b-bf7a-4366-9e35-af91c0df3174",
   "metadata": {},
   "source": [
    "# Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "856998bb-070b-4a8f-8bce-d62fc71b709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Data Processing Functions\n",
    "\n",
    "def process_dataset(df, data_path):\n",
    "    \"\"\"Process a single dataset with proper path handling\"\"\"\n",
    "    # Create output directory path\n",
    "    dataso = 'Res_Data\\\\' + data_path.split('\\\\')[1] + '\\\\' + data_path.split('\\\\')[2]\n",
    "    \n",
    "    # Convert date and set index\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    df = df.set_index('DATE')\n",
    "    \n",
    "    # Decompose time series\n",
    "    decomposition = sm.tsa.seasonal_decompose(df['PM2.5'], model='additive')\n",
    "    \n",
    "    # Create DataFrame with components\n",
    "    df1 = pd.DataFrame()\n",
    "    df1['Date'] = df.index\n",
    "    df1['Seasonality'] = decomposition.seasonal.values\n",
    "    df1['Trend'] = decomposition.trend.values\n",
    "    df1['Noise'] = decomposition.resid.values\n",
    "    df1['Original'] = df.iloc[:,-1:].values\n",
    "    \n",
    "    # Process dataframe\n",
    "    df1 = df1.iloc[12:-12,:]\n",
    "    df1 = df1.reset_index()\n",
    "    df1 = df1.iloc[:,1:]\n",
    "    df1 = df1.set_index('Date')\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(dataso, exist_ok=True)\n",
    "    \n",
    "    # Save decomposed data\n",
    "    df1.to_csv(dataso + '\\\\' + data_path.split('\\\\')[3])\n",
    "    \n",
    "    return df1, dataso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "488d60c2-6311-4b27-972d-9287271dd6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_component_data(df, component_name, dataso):\n",
    "    \"\"\"Prepare data for a specific component, checking for existing CSV files.\"\"\"\n",
    "    # Define the path for the component CSV file\n",
    "    component_file_path = os.path.join(dataso, f'{component_name}.csv')\n",
    "    \n",
    "    # Check if the component CSV file already exists\n",
    "    if os.path.exists(component_file_path):\n",
    "        print(f\"Loading existing data for {component_name} from {component_file_path}\")\n",
    "        component_df = pd.read_csv(component_file_path, index_col=0)\n",
    "    else:\n",
    "        print(f\"Creating new data for {component_name}...\")\n",
    "        # Select appropriate component data\n",
    "        if component_name == 'Seasonality':\n",
    "            component_df = df.iloc[:, :1]\n",
    "        elif component_name == 'Trend':\n",
    "            component_df = df.iloc[:, 1:2]\n",
    "        elif component_name == 'Noise':\n",
    "            component_df = df.iloc[:, 2:3]\n",
    "        else:  # Original\n",
    "            component_df = df.iloc[:, 3:4]\n",
    "        \n",
    "        # Save component data\n",
    "        component_df.to_csv(component_file_path)\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled = scaler.fit_transform(component_df.values)\n",
    "    \n",
    "    # Split data\n",
    "    train, test = train_test_split(scaled, test_size=0.30, shuffle=False)\n",
    "    train_target = train[:]\n",
    "    test_target = test[:]\n",
    "    \n",
    "    # Create generators\n",
    "    train_generator = TimeseriesGenerator(\n",
    "        train, train_target, \n",
    "        length=N_INPUT, \n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "    test_generator = TimeseriesGenerator(\n",
    "        test, test_target, \n",
    "        length=N_INPUT, \n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    return train_generator, test_generator, scaler, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bc0e1e6-5a49-4da1-bb9b-7bcabba240fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Attention Layer\n",
    "class attention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(attention,self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self,input_shape):\n",
    "        self.W = self.add_weight(name='attention_weight', shape=(input_shape[-1],1), initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(name='attention_bias', shape=(input_shape[1],1), initializer='zeros', trainable=True)        \n",
    "        super(attention, self).build(input_shape)\n",
    "    \n",
    "    def call(self,x):\n",
    "        # Alignment scores. Pass them through tanh function\n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "        # Remove dimension of size 1\n",
    "        e = K.squeeze(e, axis=-1)   \n",
    "        # Compute the weights\n",
    "        alpha = K.softmax(e)\n",
    "        # Reshape to tensorFlow format\n",
    "        alpha = K.expand_dims(alpha, axis=-1)\n",
    "        # Compute the context vector\n",
    "        context = x * alpha\n",
    "        context = K.sum(context, axis=1)\n",
    "        return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b22e1021-1524-4a17-8133-f1bf98ae51cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_grid_search(train_generator, test_generator, configs):\n",
    "    \"\"\"Perform manual grid search for hyperparameters\"\"\"\n",
    "    best_val_loss = float('inf')\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    # Try different combinations\n",
    "    for units_1 in configs['units_1']:\n",
    "        for units_2 in configs['units_2']:\n",
    "            for dropout in configs['dropout_rate']:\n",
    "                for lr in configs['learning_rate']:\n",
    "                    for reg in configs['regularization']:\n",
    "                        print(f\"\\nTrying parameters: units1={units_1}, units2={units_2}, \" f\"dropout={dropout}, lr={lr}, reg={reg}\")\n",
    "                        \n",
    "                        # Create and train model\n",
    "                        model = create_optimized_model(units_1=units_1,units_2=units_2,dropout_rate=dropout,learning_rate=lr,regularization=reg)\n",
    "                        \n",
    "                        # Early stopping\n",
    "                        es = EarlyStopping(monitor='val_loss', mode='min', patience=5, verbose=0)\n",
    "                        \n",
    "                        # Train for fewer epochs during search\n",
    "                        history = model.fit(train_generator,validation_data=test_generator,epochs=50, steps_per_epoch=STEPS_PER_EPOCH,shuffle=False,callbacks=[es],verbose=0)\n",
    "                        \n",
    "                        val_loss = min(history.history['val_loss'])\n",
    "                        \n",
    "                        if val_loss < best_val_loss:\n",
    "                            best_val_loss = val_loss\n",
    "                            best_params = {'units_1': units_1,'units_2': units_2,'dropout_rate': dropout,'learning_rate': lr,'regularization': reg}\n",
    "                            best_model = model\n",
    "                            \n",
    "                        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "                        \n",
    "    return best_params, best_model, best_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9b7def-bb24-4425-9178-4be12829eaf0",
   "metadata": {},
   "source": [
    "# Model Definition and Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5cf0bef-7ed2-4fc3-ba3f-868ea5a11b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Model Definition and Architecture\n",
    "\n",
    "def create_optimized_model(units_1=128, units_2=64, dropout_rate=0.2, \n",
    "                         learning_rate=0.001, regularization=0.001):\n",
    "    \"\"\"Create the optimized GRU model with specified hyperparameters\"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(Input(shape=(N_INPUT, N_FEATURES)))  # Define input shape here\n",
    "\n",
    "    # First GRU Layer\n",
    "    model.add(GRU(units=units_1, return_sequences=True, \n",
    "                   kernel_regularizer=l1(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Second GRU Layer\n",
    "    model.add(GRU(units=units_2, return_sequences=False, \n",
    "                   kernel_regularizer=l1(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Dense layers for final prediction\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Compile model\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f59e3ec8-dc82-45fb-a251-dc834b325480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_callbacks(model_dir, component):\n",
    "    \"\"\"Create callbacks for model training\"\"\"\n",
    "    callbacks = [\n",
    "        # Early Stopping\n",
    "        EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=12),\n",
    "        \n",
    "        # Model Checkpoint\n",
    "        ModelCheckpoint(filepath=f'{model_dir}/best_model_{component}.keras',monitor='val_loss',mode='min',save_best_only=True,verbose=1),\n",
    "        \n",
    "        # Reduce Learning Rate\n",
    "        ReduceLROnPlateau(monitor='val_loss',factor=0.2,patience=5,min_lr=0.0001,verbose=1)\n",
    "    ]\n",
    "    \n",
    "    return callbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "941b5ac6-877b-4a42-8f86-4b6daf4b0354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_predictions(prediction, pre_train, test, train, n_input, scaler):\n",
    "    \"\"\"Process model predictions\"\"\"\n",
    "    # Process test predictions\n",
    "    test1 = test[n_input:,:]\n",
    "    prediction1 = np.concatenate((prediction, test1), axis=1) if prediction.size > 0 and test1.size > 0 else np.array([])\n",
    "    rescaled_prediction = scaler.inverse_transform(prediction1) if prediction1.size > 0 else np.array([])\n",
    "    rescaled_prediction_test = rescaled_prediction[:,0] if rescaled_prediction.size > 0 else np.array([])\n",
    "    \n",
    "    # Process train predictions\n",
    "    train1 = train[n_input:,:]\n",
    "    pre_train1 = np.concatenate((pre_train, train1), axis=1) if pre_train.size > 0 and train1.size > 0 else np.array([])\n",
    "    rescaled_prediction_trai = scaler.inverse_transform(pre_train1) if pre_train1.size > 0 else np.array([])\n",
    "    rescaled_prediction_train = rescaled_prediction_trai[:,0] if rescaled_prediction_trai.size > 0 else np.array([])\n",
    "    \n",
    "    return rescaled_prediction_test, rescaled_prediction_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "620d4ea2-831b-4741-ae46-7668d20bf497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(rescaled_prediction_test, rescaled_prediction_train, \n",
    "                     component, model_dir):\n",
    "    \"\"\"Save model predictions\"\"\"\n",
    "    # Create DataFrames for predictions\n",
    "    if component == 'Original':\n",
    "        test_pr = pd.DataFrame(rescaled_prediction_test, columns=['Original_Predicted']) if rescaled_prediction_test.size > 0 else pd.DataFrame()\n",
    "        train_pr = pd.DataFrame(rescaled_prediction_train, columns=['Original_Predicted']) if rescaled_prediction_train.size > 0 else pd.DataFrame()\n",
    "    else:\n",
    "        # Load existing predictions if available\n",
    "        test_pr = pd.read_csv(f'{model_dir}/test_pre.csv', index_col=0) \\\n",
    "                 if os.path.exists(f'{model_dir}/test_pre.csv') \\\n",
    "                 else pd.DataFrame()\n",
    "        train_pr = pd.read_csv(f'{model_dir}/train_pre.csv', index_col=0) \\\n",
    "                  if os.path.exists(f'{model_dir}/train_pre.csv') \\\n",
    "                  else pd.DataFrame()\n",
    "        \n",
    "        # Add new predictions if they are not empty\n",
    "        if rescaled_prediction_test.size > 0:\n",
    "            test_pr[f'{component}_Predicted'] = rescaled_prediction_test\n",
    "        if rescaled_prediction_train.size > 0:\n",
    "            train_pr[f'{component}_Predicted'] = rescaled_prediction_train\n",
    "    \n",
    "    # Save to files with component-specific names\n",
    "    test_pr.to_csv(f'{model_dir}/{component}_test_predictions.csv', index=False)\n",
    "    train_pr.to_csv(f'{model_dir}/{component}_train_predictions.csv', index=False)\n",
    "    \n",
    "    return test_pr, train_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b12973a2-2ba9-4beb-a1f6-fec77207365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_history(history, model_dir, component):\n",
    "    \"\"\"Save model training history\"\"\"\n",
    "    hist_df = pd.DataFrame(history.history)\n",
    "    hist_df.to_csv(f'{model_dir}/{component}_history.csv')\n",
    "    \n",
    "    # Create and save training plots\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{component} Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot MAE\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='Training MAE')\n",
    "    plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "    plt.title(f'{component} Model MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{model_dir}/{component}_training_history.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebe56f2-95b5-4b80-8ddb-1f89bb9edae4",
   "metadata": {},
   "source": [
    "# Training and Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6996b4b8-2ac8-4d70-9dd0-24edee2fb4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset 1/17\n",
      "\u001b[31mDATASET----------------../DataSets\\GUJARAT\\ANKLESHWAR\\H_Ankl_1_2_19-3_12_22-_41_ (copy)_INDEX_Mean.csv\n",
      "\n",
      "Processing Seasonality component...\n",
      "Data shapes - Train: (23457, 1), Test: (10054, 1)\n",
      "Performing hyperparameter search...\n",
      "\n",
      "Trying parameters: units1=64, units2=32, dropout=0.1, lr=0.001, reg=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vp123\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    }
   ],
   "source": [
    "# Section 4: Training and Evaluation Loop\n",
    "\n",
    "# Define components to process\n",
    "components = ['Seasonality', 'Trend', 'Noise', 'Original']\n",
    "\n",
    "# Main training loop\n",
    "for ds in range(len(dh)):\n",
    "    print(f\"\\nProcessing dataset {ds+1}/{len(dh)}\")\n",
    "    print(FORES[ds % len(FORES)] + f'DATASET----------------{dh[ds]}')\n",
    "    \n",
    "    try:\n",
    "        # Process dataset\n",
    "        df1, dataso = process_dataset(li[ds], dh[ds])\n",
    "        \n",
    "        # Create model directory\n",
    "        model_dir = dataso + '/' + MODEL_NAME\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "        # Store results for this dataset\n",
    "        dataset_results = {\n",
    "            'dataset': dh[ds],\n",
    "            'components': {}\n",
    "        }\n",
    "        \n",
    "        # Process each component\n",
    "        for component in components:\n",
    "            print(f\"\\nProcessing {component} component...\")\n",
    "            \n",
    "            try:\n",
    "                # Prepare data\n",
    "                train_generator, test_generator, scaler, train, test = prepare_component_data(df1, component, dataso)\n",
    "                \n",
    "                print(f\"Data shapes - Train: {train.shape}, Test: {test.shape}\")\n",
    "                \n",
    "                # Perform grid search\n",
    "                print(\"Performing hyperparameter search...\")\n",
    "                best_params, best_model, best_val_loss = simple_grid_search(train_generator, test_generator, model_configs)\n",
    "                \n",
    "                print(\"\\nBest parameters found:\")\n",
    "                for param, value in best_params.items():\n",
    "                    print(f\"{param}: {value}\")\n",
    "                print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "                \n",
    "                # Create callbacks\n",
    "                callbacks = create_callbacks(model_dir, component)\n",
    "                \n",
    "                # Train final model with best parameters\n",
    "                print(\"\\nTraining final model with best parameters...\")\n",
    "                final_model = create_optimized_model(**best_params)\n",
    "                \n",
    "                # Train model\n",
    "                history = final_model.fit(train_generator,validation_data=test_generator,epochs=N_EPOCHS,steps_per_epoch=STEPS_PER_EPOCH,shuffle=False,callbacks=callbacks,verbose=1)\n",
    "                \n",
    "                # Generate predictions\n",
    "                print(\"Generating predictions...\")\n",
    "                prediction = final_model.predict(test_generator)\n",
    "                pre_train = final_model.predict(train_generator)\n",
    "                \n",
    "                # Process predictions\n",
    "                rescaled_prediction_test, rescaled_prediction_train = process_predictions(prediction, pre_train, test, train, N_INPUT, scaler)\n",
    "                \n",
    "                # Save predictions\n",
    "                # test_pr, train_pr = save_predictions(rescaled_prediction_test,rescaled_prediction_train,component,model_dir)\n",
    "                # Example call to save_predictions\n",
    "                # test_pr, train_pr = save_predictions(rescaled_prediction_test, rescaled_prediction_train, component, model_dir, test, train)\n",
    "                # Example call to save_predictions within your training loop\n",
    "                try:\n",
    "                    # Assuming rescaled_prediction_test and rescaled_prediction_train are defined\n",
    "                    test_pr, train_pr = save_predictions(rescaled_prediction_test, rescaled_prediction_train, component, model_dir)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {component} component: {str(e)}\")\n",
    "                # Before calling save_predictions\n",
    "                print(f\"Shapes - Test Predictions: {rescaled_prediction_test.shape}, Train Predictions: {rescaled_prediction_train.shape}\")\n",
    "\n",
    "                # Call save_predictions\n",
    "                test_pr, train_pr = save_predictions(rescaled_prediction_test, rescaled_prediction_train, component, model_dir)\n",
    "\n",
    "\n",
    "                # Save model history\n",
    "                save_model_history(history, model_dir, component)\n",
    "                \n",
    "                # Store component results\n",
    "                dataset_results['components'][component] = {'best_params': best_params,'best_val_loss': best_val_loss,'final_val_loss': history.history['val_loss'][-1],'final_val_mae': history.history['val_mae'][-1]}\n",
    "                \n",
    "                print(f\"Completed {component} component\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {component} component: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Save dataset results summary\n",
    "        results_df = pd.DataFrame.from_dict(\n",
    "            {(i,j): dataset_results['components'][i][j] \n",
    "             for i in dataset_results['components'].keys() \n",
    "             for j in dataset_results['components'][i].keys()},\n",
    "            orient='index'\n",
    "        )\n",
    "        results_df.to_csv(f'{model_dir}/model_summary.csv')\n",
    "        \n",
    "        # Create summary plots\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Plot final validation losses\n",
    "        plt.subplot(1, 2, 1)\n",
    "        val_losses = [data['final_val_loss'] \n",
    "                     for data in dataset_results['components'].values()]\n",
    "        plt.bar(components, val_losses)\n",
    "        plt.title('Final Validation Loss by Component')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.ylabel('Loss')\n",
    "        \n",
    "        # Plot final MAE\n",
    "        plt.subplot(1, 2, 2)\n",
    "        val_maes = [data['final_val_mae'] \n",
    "                   for data in dataset_results['components'].values()]\n",
    "        plt.bar(components, val_maes)\n",
    "        plt.title('Final Validation MAE by Component')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.ylabel('MAE')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{model_dir}/final_metrics_summary.png')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Completed dataset {ds+1}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing dataset {ds}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\nTraining completed for all datasets!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86cc302-17d6-4a13-bc23-da6f99ef2f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename ='AO_Gru'\n",
    "Pre_Ds='Res_Data\\\\'\n",
    "vip=f'jupyter nbconvert --to html {filename}.ipynb --stdout > {Pre_Ds}{MODEL_NAME}.html'\n",
    "os.system(vip)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
